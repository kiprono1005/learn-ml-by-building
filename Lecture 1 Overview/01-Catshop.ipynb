{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5806f33a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CatShop: Learning Machine Learning Through a Cat's Eyes üê±\n",
    "*A Beginner's Guide to Three Core ML Approaches*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bac5cfb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Today's Learning Goals\n",
    "\n",
    "We're going to explore machine learning by building **CatShop** - an e-commerce system that thinks like a cat!\n",
    "\n",
    "**Why cats?** \n",
    "- Makes abstract concepts concrete and fun\n",
    "- Forces us to think about perspective (a key ML skill!)\n",
    "- Shows how we transform data for specific tasks\n",
    "\n",
    "**What you'll learn:**\n",
    "1. **Supervised Learning**: Teaching computers with examples\n",
    "2. **Unsupervised Learning**: Finding patterns without labels  \n",
    "3. **Active Learning**: Being smart about what to label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11312ce",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Big Picture\n",
    "\n",
    "Think of ML like teaching a child:\n",
    "- **Supervised**: \"This is a dog, this is a cat\" (lots of examples)\n",
    "- **Unsupervised**: \"Group these animals by similarity\" (no labels)\n",
    "- **Active**: \"Which animal are you unsure about?\" (strategic learning)\n",
    "\n",
    "Today we'll use a real language model (Gemma-3) to experience all three!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c0c30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: Setting Up Our Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad0514f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02022fd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# RISE Configuration Cell (Run this first)\n",
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure RISE settings\n",
    "path = Path.home() / \".jupyter\" / \"nbconfig\"\n",
    "cm = BaseJSONConfigManager(config_dir=str(path))\n",
    "cm.update('livereveal', {\n",
    "    'scroll': True,  # Enable scrolling\n",
    "    'width': 1024,\n",
    "    'height': 768,\n",
    "    'start_slideshow_at': 'beginning',\n",
    "    'theme': 'white',  # Clean theme for teaching\n",
    "    'transition': 'none',  # No distracting transitions\n",
    "    'enable_chalkboard': True,  # For annotations during lecture\n",
    "    'autolaunch': False\n",
    "})\n",
    "\n",
    "print(\"‚úÖ RISE configured for teaching presentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d10b93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def get_optimal_device():\n",
    "    \"\"\"Get the best available device with proper fallback\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        # Check if MPS is actually built and functional\n",
    "        try:\n",
    "            # Test MPS with a small tensor\n",
    "            test = torch.tensor([1.0]).to(\"mps\")\n",
    "            _ = test * 2\n",
    "            return torch.device(\"mps\")\n",
    "        except:\n",
    "            print(\"MPS available but not functional, using CPU\")\n",
    "            return torch.device(\"cpu\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Set device globally\n",
    "device = get_optimal_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# os.environ['FORCE_CPU'] = '1'  # STUDENT: Add this line to force CPU\n",
    "if os.environ.get('FORCE_CPU') == '1':\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Forced to CPU mode for compatibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d0b639",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "%pip install -U \"transformers>=4.44\" datasets evaluate accelerate peft\n",
    "%pip install -q torch scikit-learn matplotlib pandas\n",
    "%pip install -q safetensors\n",
    "\n",
    "import torch, transformers, peft, accelerate, numpy\n",
    "from transformers import AutoTokenizer\n",
    "print(torch.__version__)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, silhouette_score, davies_bouldin_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create directory structure\n",
    "Path('data').mkdir(exist_ok=True)\n",
    "Path('data/config').mkdir(parents=True, exist_ok=True)\n",
    "Path('data/processed').mkdir(exist_ok=True)\n",
    "Path('models').mkdir(exist_ok=True)\n",
    "Path('models/gemma-cat-lora').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üê± Welcome to CatShop ML Tutorial!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd351c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Great! We've imported our tools. Notice we're using PEFT (Parameter-Efficient Fine-Tuning) - this is a modern technique that lets us fine-tune large models quickly by only training a small number of parameters. Think of it like teaching a skilled chef a new cuisine - we don't retrain everything they know about cooking, just add the new knowledge on top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae249935",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Loading and Understanding Our Data\n",
    "\n",
    "In machine learning, data is everything. \n",
    "\n",
    "**The Challenge:** We have normal product descriptions. We need to teach the computer how a cat would see them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f1725a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load e-commerce products\n",
    "with open('data/items_shuffle_1000.json', 'r') as f:\n",
    "    items = json.load(f)\n",
    "\n",
    "print(f\"üì¶ We have {len(items)} products to work with\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e03cd41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### How Cats See the World üê±\n",
    "\n",
    "**Key Insight:** Same product, different perspective!\n",
    "- Human sees: \"Laptop computer\"\n",
    "- Cat sees: \"Warm napping surface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa6abf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Define the cat's worldview\n",
    "CAT_CATEGORIES = {\n",
    "    \"NAP_SURFACE\": \"Things to sleep on\",\n",
    "    \"HUNT_PLAY\": \"Things to chase\",\n",
    "    \"TERRITORY\": \"Things to claim\",\n",
    "    \"GROOMING\": \"Self-care items\",\n",
    "    \"CONSUMPTION\": \"Food and water\",\n",
    "    \"DANGER\": \"SCARY THINGS!\",\n",
    "    \"IRRELEVANT\": \"Boring human stuff\"\n",
    "}\n",
    "\n",
    "CATEGORY_TO_ID = {cat: i for i, cat in enumerate(CAT_CATEGORIES.keys())}\n",
    "ID_TO_CATEGORY = {i: cat for cat, i in CATEGORY_TO_ID.items()}\n",
    "\n",
    "# Use consistently throughout:\n",
    "# - Use CAT_CATEGORIES.keys() when iterating over category names\n",
    "# - Use CATEGORY_TO_ID[category] to get ID from category name\n",
    "# - Use ID_TO_CATEGORY[id] to get category name from ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9052489",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "Notice how we're transforming the problem. Instead of traditional e-commerce categories, we're creating a new taxonomy based on cat behavior. \n",
    "\n",
    "üí° **This is Feature Engineering:** Creating useful ways to represent data!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd812c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3: Data Transformation - Teaching Machines to Think Like Cats\n",
    "\n",
    "Now comes the interesting part. We need to transform human product descriptions into cat perspectives. We'll use a comprehensive rule set that you've prepared using an LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db90acda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import json, os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# 1) Load datasets\n",
    "with open('data/items_shuffle_1000.json', 'r') as f:\n",
    "    raw = json.load(f)\n",
    "items_list = raw if isinstance(raw, list) else list(raw.values())\n",
    "\n",
    "# 2) Load rules\n",
    "with open('data/config/cat_mapping_rules.json', 'r') as f:\n",
    "    keyword_rules = json.load(f)  # keyword -> cat label\n",
    "\n",
    "# Optional: category mapping (top-level breadcrumb -> cat label)\n",
    "top_map_path = Path('data/config/webshop_top_to_cat.json')\n",
    "top_map = {}\n",
    "if top_map_path.exists():\n",
    "    with top_map_path.open('r') as f:\n",
    "        top_map = json.load(f)\n",
    "\n",
    "\n",
    "def top_level_from_breadcrumb(breadcrumb: str):\n",
    "    if not isinstance(breadcrumb, str) or not breadcrumb.strip():\n",
    "        return None\n",
    "    for sep in [\"‚Ä∫\", \">\", \"/\"]:\n",
    "        if sep in breadcrumb:\n",
    "            parts = [p.strip() for p in breadcrumb.split(sep)]\n",
    "            for p in parts:\n",
    "                if p:\n",
    "                    return p\n",
    "    return breadcrumb.strip()\n",
    "\n",
    "def build_text(it):\n",
    "    fields = [\n",
    "        it.get(\"name\",\"\"),\n",
    "        it.get(\"title\",\"\"),\n",
    "        it.get(\"category\",\"\"),\n",
    "        it.get(\"product_category\",\"\"),\n",
    "        it.get(\"small_description_old\",\"\"),\n",
    "        it.get(\"full_description\",\"\"),\n",
    "    ]\n",
    "    return \" \".join(f for f in fields if isinstance(f, str)).lower()\n",
    "\n",
    "def label_item(it):\n",
    "    # 1) Category-based label (preferred if available)\n",
    "    tl = top_level_from_breadcrumb(it.get(\"product_category\", \"\"))\n",
    "    if tl and tl in top_map:\n",
    "        return top_map[tl]\n",
    "\n",
    "    # 2) Keyword-based label (longest match wins)\n",
    "    text = build_text(it)\n",
    "    cat, max_len = \"IRRELEVANT\", 0\n",
    "    for kw, label in keyword_rules.items():\n",
    "        k = kw.lower()\n",
    "        if k and k in text and len(k) > max_len:\n",
    "            cat, max_len = label, len(k)\n",
    "    return cat\n",
    "\n",
    "# 3) Transform all products\n",
    "cat_products = []\n",
    "for it in items_list:\n",
    "    name = it.get(\"name\") or it.get(\"title\") or \"Unknown\"\n",
    "    cat = label_item(it)\n",
    "    cat_products.append({\n",
    "        \"name\": name[:200],\n",
    "        \"cat_category\": cat,\n",
    "        \"cat_category_id\": CATEGORY_TO_ID[cat]\n",
    "    })\n",
    "\n",
    "# 4) Analyze + save\n",
    "dist = Counter([p['cat_category'] for p in cat_products])\n",
    "print(\"üìä Category distribution:\")\n",
    "for cat, count in dist.most_common():\n",
    "    pct = count / len(cat_products) * 100\n",
    "    print(f\"  {cat:12s}: {count:4d} ({pct:5.1f}%)\")\n",
    "\n",
    "Path('data/processed').mkdir(parents=True, exist_ok=True)\n",
    "with open('data/processed/cat_products.json', 'w') as f:\n",
    "    json.dump(cat_products, f, indent=2)\n",
    "print(f\"‚úÖ Saved {len(cat_products)} transformed products -> data/processed/cat_products.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eedf5d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "This rule-based labeling is our starting point. In industry, this is called **weak supervision** - using heuristics to create initial labels that we can refine with ML. The comprehensive rules ensure better coverage across all product categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cb9c17",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Quick diagnostics to validate robustness\n",
    "\n",
    "Where did each label come from (category vs keywords)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33198462",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "items = json.loads(Path(\"data/items_shuffle_1000.json\").read_text())\n",
    "items = items if isinstance(items, list) else list(items.values())\n",
    "rules = json.loads(Path(\"data/config/cat_mapping_rules.json\").read_text())\n",
    "top_map = json.loads(Path(\"data/config/webshop_top_to_cat.json\").read_text())\n",
    "\n",
    "def top_level(bc):\n",
    "    if not isinstance(bc, str): return None\n",
    "    for sep in [\"‚Ä∫\", \">\", \"/\"]:\n",
    "        if sep in bc:\n",
    "            for p in [s.strip() for s in bc.split(sep)]:\n",
    "                if p: return p\n",
    "    return bc.strip() or None\n",
    "\n",
    "def build_text(it):\n",
    "    fields = [it.get(\"name\",\"\"), it.get(\"title\",\"\"), it.get(\"category\",\"\"), it.get(\"product_category\",\"\"), it.get(\"small_description_old\",\"\"), it.get(\"full_description\",\"\")]\n",
    "    return \" \".join([f for f in fields if isinstance(f, str)]).lower()\n",
    "\n",
    "src_counter = Counter()\n",
    "label_counter = Counter()\n",
    "examples = {k: [] for k in [\"category\",\"keyword\"]}\n",
    "\n",
    "for it in items:\n",
    "    tl = top_level(it.get(\"product_category\",\"\"))\n",
    "    if tl in top_map:\n",
    "        label = top_map[tl]\n",
    "        src = \"category\"\n",
    "    else:\n",
    "        text = build_text(it)\n",
    "        label, max_len = \"IRRELEVANT\", 0\n",
    "        match_kw = None\n",
    "        for kw, lab in rules.items():\n",
    "            k = kw.lower()\n",
    "            if k and k in text and len(k) > max_len:\n",
    "                label, max_len, match_kw = lab, len(k), kw\n",
    "        src = \"keyword\"\n",
    "    src_counter[src] += 1\n",
    "    label_counter[label] += 1\n",
    "    if len(examples[src]) < 3:\n",
    "        examples[src].append((it.get(\"name\") or it.get(\"title\") or \"Unknown\", tl if src==\"category\" else match_kw, label))\n",
    "\n",
    "print(\"By source:\", src_counter)\n",
    "print(\"By label:\", label_counter)\n",
    "print(\"Examples (category):\", examples[\"category\"])\n",
    "print(\"Examples (keyword):\", examples[\"keyword\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d5c92",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Part 3.5: Preparing Rich Training Data\n",
    "\n",
    "To train our model effectively, we need diverse training examples. Let's load the pre-generated conversational and explanation data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114b0a6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Generate and load training data (Gemma-3-270m)\n",
    "\n",
    "We generate two datasets for instruction-style finetuning 'Lecture 1 Overview/data/generate_training_data.py':\n",
    "- `conversation_examples.json`: short cat-thought responses given product names.\n",
    "- `explanation_examples.json`: brief rationales for the assigned cat category.\n",
    "\n",
    "If the data already exists, we just load and summarize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340dcbb0",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "processed_dir = Path(\"data/processed\")  # <-- corrected\n",
    "\n",
    "with open(processed_dir / \"conversation_examples.json\") as f:\n",
    "    conversation_examples = json.load(f)\n",
    "\n",
    "with open(processed_dir / \"explanation_examples.json\") as f:\n",
    "    explanation_examples = json.load(f)\n",
    "\n",
    "print(\"‚úÖ Loaded datasets\")\n",
    "print(f\"- Conversations: {len(conversation_examples)}\")\n",
    "print(f\"- Explanations: {len(explanation_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd5ec56",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Adjust this if your working directory is repo root:\n",
    "# processed_dir = Path(\"Lecture 1 Overview/data/processed\")\n",
    "processed_dir = Path(\"data/processed\")\n",
    "\n",
    "with open(processed_dir / \"conversation_examples.json\") as f:\n",
    "    conversation_examples = json.load(f)\n",
    "\n",
    "with open(processed_dir / \"explanation_examples.json\") as f:\n",
    "    explanation_examples = json.load(f)\n",
    "\n",
    "print(\"‚úÖ Loaded datasets\")\n",
    "print(f\"- Conversations: {len(conversation_examples)}\")\n",
    "print(f\"- Explanations: {len(explanation_examples)}\")\n",
    "\n",
    "def show_examples(examples, n=3, title=\"Examples\"):\n",
    "    print(f\"\\n--- {title} (showing {n}) ---\")\n",
    "    for ex in examples[:n]:\n",
    "        mode = 'conversation' if 'conversation' in ex else ('explanation' if 'explanation' in ex else None)\n",
    "        prompt = ex.get(mode, {}).get('prompt', '') if mode else ''\n",
    "        completion = ex.get(mode, {}).get('completion', '') if mode else ''\n",
    "        print(f\"- Product: {ex.get('product_name','')}\")\n",
    "        print(f\"  Category: {ex.get('category','')}\")\n",
    "        print(f\"  Prompt: {prompt[:140]}...\")\n",
    "        print(f\"  Completion: {completion[:200]}...\\n\")\n",
    "\n",
    "show_examples(conversation_examples, n=3, title=\"Conversation examples\")\n",
    "show_examples(explanation_examples, n=3, title=\"Explanation examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e1071",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Having diverse training data - classification, conversation, and explanation - helps the model learn multiple aspects of the task while maintaining its general capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb1676d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Part 4: Building Our Gemma-3 Cat Classifier\n",
    "\n",
    "Now we'll load Gemma-3, a powerful but efficient language model from Google. We'll use it as our base model and teach it to think like a cat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34391d1b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "class GemmaCatClassifier:\n",
    "    \"\"\"\n",
    "    A classifier that uses Gemma-3 to categorize products from a cat's perspective.\n",
    "    Uses PEFT (LoRA) for efficient fine-tuning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"google/gemma-3-270m\", use_lora=True, checkpoint_path=None):\n",
    "        print(f\"üîß Initializing {model_name}...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Consistent device handling\n",
    "        self.device = device  # Use global device\n",
    "        \n",
    "        # Load model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            attn_implementation=\"eager\", #For Gemma-3, prefer eager attention\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "        )\n",
    "        \n",
    "        # Move to device after loading\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        # Check available modules for LoRA\n",
    "        if use_lora and checkpoint_path is None:\n",
    "            print(\"üîç Checking available modules for LoRA...\")\n",
    "            available_modules = [n for n, _ in self.model.named_modules() \n",
    "                               if any(key in n for key in ['proj', 'gate', 'fc'])]\n",
    "            print(f\"  Found modules: {available_modules[:5]}...\")  # Show first 5\n",
    "            \n",
    "            # Apply LoRA for efficient fine-tuning\n",
    "            print(\"üìé Applying LoRA for efficient fine-tuning...\")\n",
    "            peft_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                inference_mode=False,\n",
    "                r=8,  # rank\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Gemma modules\n",
    "            )\n",
    "            self.model = get_peft_model(self.model, peft_config)\n",
    "            trainable, total = self.model.get_nb_trainable_parameters()\n",
    "            pct = 100 * trainable / total\n",
    "            print(f\"Trainable params: {trainable:,} || all params: {total:,} || trainable%: {pct:.4f}\")\n",
    "        \n",
    "        # Load from checkpoint if provided\n",
    "        if checkpoint_path:\n",
    "            print(f\"üìÇ Loading checkpoint from {checkpoint_path}\")\n",
    "            from peft import PeftModel\n",
    "            self.model = PeftModel.from_pretrained(self.model, checkpoint_path)\n",
    "        \n",
    "        self.device = next(self.model.parameters()).device\n",
    "        \n",
    "        # Define category tokens for classification\n",
    "        self.cat_tokens = {\n",
    "            'NAP_SURFACE': 'nap', 'HUNT_PLAY': 'hunt',\n",
    "            'TERRITORY': 'territory', 'DANGER': 'danger',\n",
    "            'CONSUMPTION': 'food', 'GROOMING': 'groom',\n",
    "            'IRRELEVANT': 'boring'\n",
    "        }\n",
    "        \n",
    "        # Get token IDs\n",
    "        self.category_token_ids = {}\n",
    "        for cat, token in self.cat_tokens.items():\n",
    "            token_ids = self.tokenizer.encode(token, add_special_tokens=False)\n",
    "            self.category_token_ids[cat] = token_ids[0]\n",
    "    \n",
    "    def classify(self, product_name, return_probs=False):\n",
    "        \"\"\"\n",
    "        Classify a product using language model probabilities.\n",
    "        This is the key insight: we use the model's next-token predictions!\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        device = next(self.model.parameters()).device\n",
    "        prompt = f\"Question: How would a cat categorize '{product_name}'?\\nAnswer: This is\" # Create prompt\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device) # Tokenize\n",
    "        self.model.eval()\n",
    "        \n",
    "        \n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            next_token_logits = outputs.logits[0, -1, :]\n",
    "            \n",
    "            # Extract logits for our category tokens\n",
    "            category_logits = []\n",
    "            for cat in CAT_CATEGORIES.keys():\n",
    "                token_id = self.category_token_ids[cat]\n",
    "                category_logits.append(next_token_logits[token_id])\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probs = F.softmax(torch.stack(category_logits), dim=0)\n",
    "        \n",
    "        # Get prediction\n",
    "        pred_idx = torch.argmax(probs).item()\n",
    "        pred_category = list(CAT_CATEGORIES.keys())[pred_idx]\n",
    "        \n",
    "        if return_probs:\n",
    "            prob_dict = {cat: p.item() for cat, p in zip(CAT_CATEGORIES.keys(), probs)}\n",
    "            return pred_category, prob_dict\n",
    "        return pred_category\n",
    "    \n",
    "    def get_uncertainty(self, product_name):\n",
    "        \"\"\"Calculate uncertainty using entropy of probability distribution\"\"\"\n",
    "        _, probs = self.classify(product_name, return_probs=True)\n",
    "        # Calculate entropy\n",
    "        entropy = -sum(p * np.log(p + 1e-10) for p in probs.values() if p > 0)\n",
    "        return entropy\n",
    "    \n",
    "    def get_embeddings(self, texts, batch_size=8):\n",
    "        \"\"\"Extract embeddings for unsupervised learning analysis\"\"\"\n",
    "        embeddings = []\n",
    "\n",
    "        # Check if we're on MPS and using PEFT - if so, temporarily use CPU\n",
    "        is_mps = str(next(self.model.parameters()).device).startswith('mps')\n",
    "        is_peft = hasattr(self.model, 'peft_config')\n",
    "\n",
    "        if is_mps and is_peft:\n",
    "            # Temporarily move to CPU for embedding extraction\n",
    "            print(\"üìç Note: Using CPU for embedding extraction (MPS+PEFT compatibility)\")\n",
    "            original_device = next(self.model.parameters()).device\n",
    "            self.model = self.model.to('cpu')\n",
    "            compute_device = 'cpu'\n",
    "        else:\n",
    "            compute_device = self.device\n",
    "            original_device = None\n",
    "\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = self.tokenizer(batch, return_tensors=\"pt\", \n",
    "                                   padding=True, truncation=True, max_length=256).to(compute_device)\n",
    "\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs, output_hidden_states=True)\n",
    "                # Use mean pooling of last hidden state\n",
    "                hidden = outputs.hidden_states[-1]\n",
    "                mask = inputs.attention_mask.unsqueeze(-1)\n",
    "                masked_hidden = hidden * mask\n",
    "                summed = masked_hidden.sum(dim=1)\n",
    "                counts = mask.sum(dim=1)\n",
    "                mean_pooled = summed / counts\n",
    "                embeddings.extend(mean_pooled.cpu().numpy())\n",
    "\n",
    "        # Move model back to original device if we switched\n",
    "        if original_device is not None:\n",
    "            self.model = self.model.to(original_device)\n",
    "\n",
    "        return np.array(embeddings)\n",
    "\n",
    "# Initialize our classifier\n",
    "classifier = GemmaCatClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03367e13",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open(\"data/processed/cat_products.json\") as f:\n",
    "    cat_products = json.load(f)\n",
    "\n",
    "train_products, val_products = train_test_split(\n",
    "    cat_products,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=[p['cat_category_id'] for p in cat_products]\n",
    ")\n",
    "\n",
    "print(f\"Validation size: {len(val_products)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eb2235",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model_capabilities(classifier, test_products_sample):\n",
    "    \"\"\"Evaluate both classification and conversation abilities\"\"\"\n",
    "    \n",
    "    # Classification accuracy on validation set\n",
    "    correct = 0\n",
    "    for product in test_products_sample[:50]:\n",
    "        pred = classifier.classify(product['name'])\n",
    "        if pred == product['cat_category']:\n",
    "            correct += 1\n",
    "    accuracy = correct / 50\n",
    "\n",
    "    # Test on specific example products\n",
    "    test_products = [\"laptop computer\", \"cardboard box\", \"cat toy\", \"vacuum cleaner\"]\n",
    "    test_results = []\n",
    "    for product in test_products:\n",
    "        pred, probs = classifier.classify(product, return_probs=True)\n",
    "        confidence = probs[pred]\n",
    "        test_results.append({\n",
    "            'product': product,\n",
    "            'prediction': pred,\n",
    "            'confidence': confidence,\n",
    "            'probs': probs\n",
    "        })\n",
    "\n",
    "    # Better prompts for conversation\n",
    "    test_prompts = [\n",
    "        \"You are a cat expert. Question: Why do cats love boxes? Answer:\",\n",
    "        \"You are a cat behavior specialist. Question: My cat keeps knocking things off the table. What's she thinking? Answer:\",\n",
    "        \"From a cat's perspective, explain why a laptop is seen as a napping surface:\"\n",
    "    ]\n",
    "\n",
    "    responses = []\n",
    "    classifier.model.eval()\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        inputs = classifier.tokenizer(\n",
    "            prompt, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=256\n",
    "        ).to(classifier.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = classifier.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=classifier.tokenizer.pad_token_id,\n",
    "                eos_token_id=classifier.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # Decode only newly generated tokens\n",
    "        gen_ids = outputs[0, inputs[\"input_ids\"].shape[-1]:]\n",
    "        response = classifier.tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "        \n",
    "        # Clean up response\n",
    "        response = response.replace(\"Human:\", \"\").replace(\"Assistant:\", \"\").strip()\n",
    "        if \"\\n\" in response:\n",
    "            response = response.split(\"\\n\")[0]\n",
    "        \n",
    "        responses.append(response)\n",
    "    \n",
    "    return accuracy, test_results, responses\n",
    "\n",
    "def display_model_evaluation(accuracy, test_results, responses):\n",
    "    \"\"\"Display evaluation results in a clear, comprehensive format\"\"\"\n",
    "    \n",
    "    # Overall accuracy\n",
    "    print(f\"\\nüìà Validation Accuracy: {accuracy:.1%}\")\n",
    "    \n",
    "    # Test products classification\n",
    "    print(\"\\nüß™ Test Products Classification:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for result in test_results:\n",
    "        product = result['product']\n",
    "        pred = result['prediction']\n",
    "        confidence = result['confidence']\n",
    "        \n",
    "        # Create confidence bar\n",
    "        bar_length = int(confidence * 20)\n",
    "        confidence_bar = '‚ñà' * bar_length + '‚ñë' * (20 - bar_length)\n",
    "        \n",
    "        print(f\"  '{product:20s}' ‚Üí {pred:12s} [{confidence_bar}] {confidence:.1%}\")\n",
    "    \n",
    "    # Show confidence distribution for interesting cases\n",
    "    print(\"\\nüìä Confidence Distribution (for 'laptop computer'):\")\n",
    "    if test_results:\n",
    "        laptop_result = next((r for r in test_results if r['product'] == 'laptop computer'), None)\n",
    "        if laptop_result and 'probs' in laptop_result:\n",
    "            probs = laptop_result['probs']\n",
    "            # Sort by probability\n",
    "            sorted_cats = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "            for cat, prob in sorted_cats:\n",
    "                bar_length = int(prob * 10)\n",
    "                bar = '‚ñ∏' * bar_length + '¬∑' * (10 - bar_length)\n",
    "                print(f\"    {cat:12s}: [{bar}] {prob:.1%}\")\n",
    "    \n",
    "    # Conversation quality\n",
    "    print(\"\\nüí¨ Conversation Quality Samples:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    questions = [\n",
    "        \"Why do cats love boxes?\",\n",
    "        \"Why do cats knock things off tables?\",\n",
    "        \"Why is a laptop a napping surface?\"\n",
    "    ]\n",
    "    \n",
    "    for i, (q, r) in enumerate(zip(questions, responses)):\n",
    "        print(f\"\\n  Q{i+1}: {q}\")\n",
    "        # Truncate and clean response\n",
    "        clean_response = r[:100] if len(r) > 100 else r\n",
    "        if len(r) > 100:\n",
    "            clean_response = clean_response.rsplit(' ', 1)[0] + \"...\"\n",
    "        print(f\"  üê±: {clean_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83031d1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Baseline: untrained base model (no LoRA)\n",
    "baseline_classifier = GemmaCatClassifier(use_lora=False)\n",
    "\n",
    "# Ensure tokenizer pad and device consistency\n",
    "if baseline_classifier.tokenizer.pad_token is None:\n",
    "    baseline_classifier.tokenizer.pad_token = baseline_classifier.tokenizer.eos_token\n",
    "baseline_classifier.model.to(baseline_classifier.device)\n",
    "\n",
    "print(\"\\nüìä Untrained baseline (no LoRA)\")\n",
    "print(\"-\"*40)\n",
    "acc0, testres0, resp0 = evaluate_model_capabilities(baseline_classifier, val_products)\n",
    "display_model_evaluation(acc0, testres0, resp0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a11134a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Notice how we're using the language model for classification? Instead of adding a classification head, we're checking which category token the model thinks is most likely to come next. This is more flexible and maintains the model's conversational abilities!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0bf9b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 5: Supervised Learning - Fine-tuning with LoRA\n",
    "\n",
    "Time for our first paradigm: **Supervised Learning**. We'll fine-tune Gemma-3 using our labeled data. Thanks to LoRA, this will be fast and memory-efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b185fbe2",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "\n",
    "class CatProductDataset(Dataset):\n",
    "    \"\"\"Dataset for training our cat classifier\"\"\"\n",
    "    \n",
    "    def __init__(self, products, conversation_examples, explanation_examples, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.examples = []\n",
    "        \n",
    "        # Add classification examples\n",
    "        for product in products:\n",
    "            self.examples.append({\n",
    "                'input': f\"Question: How would a cat categorize '{product['name']}'?\\nAnswer: This is\",\n",
    "                'output': f\" {classifier.cat_tokens[product['cat_category']]}\",\n",
    "                'category_id': product['cat_category_id']\n",
    "            })\n",
    "            \n",
    "        # Add conversation examples\n",
    "        for conv in conversation_examples[:len(products)//3]:  # Add 1/3 as many\n",
    "            block = conv.get('conversation', conv)  # support nested or flat\n",
    "            prompt = block.get('prompt')\n",
    "            completion = block.get('completion')\n",
    "            if not prompt or not completion:\n",
    "                continue\n",
    "            self.examples.append({\n",
    "                'input': prompt,\n",
    "                'output': completion,\n",
    "                'category_id': CATEGORY_TO_ID.get(conv.get('category'), 6)\n",
    "            })\n",
    "\n",
    "        # Add explanation examples\n",
    "        for expl in explanation_examples[:len(products)//3]:  # Add 1/3 as many\n",
    "            block = expl.get('explanation', expl)  # support nested or flat\n",
    "            prompt = block.get('prompt')\n",
    "            completion = block.get('completion')\n",
    "            if not prompt or not completion:\n",
    "                continue\n",
    "            self.examples.append({\n",
    "                'input': prompt,\n",
    "                'output': completion,\n",
    "                'category_id': CATEGORY_TO_ID.get(expl.get('category'), 6)\n",
    "            })\n",
    "\n",
    "\n",
    "        \n",
    "        print(f\"  Created dataset with {len(self.examples)} total examples\")\n",
    "        print(f\"    - Classification: {len(products)}\")\n",
    "        print(f\"    - Conversations: {min(len(conversation_examples), len(products)//3)}\")\n",
    "        print(f\"    - Explanations: {min(len(explanation_examples), len(products)//3)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        example = self.examples[idx]\n",
    "        full_text = example['input'] + example['output']\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Create labels (mask the input part for loss calculation)\n",
    "        labels = encoding['input_ids'].clone()\n",
    "        input_length = len(self.tokenizer.encode(\n",
    "            example['input'],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            add_special_tokens=False  # avoid extra BOS/EOS affecting mask\n",
    "        ))\n",
    "        labels[0, :input_length] = -100  # Don't compute loss on input\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': labels.squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ef19b6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_supervised_model(classifier, cat_products, conversation_examples, explanation_examples, epochs=2):\n",
    "    \"\"\"\n",
    "    Fine-tune the model using supervised learning.\n",
    "    This is where the magic happens!\n",
    "    \"\"\"\n",
    "    print(\"\\nüéØ SUPERVISED LEARNING: Fine-tuning Gemma-3\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Split data\n",
    "    train_products, val_products = train_test_split(\n",
    "        cat_products, test_size=0.2, random_state=42,\n",
    "        stratify=[p['cat_category_id'] for p in cat_products]\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Dataset split:\")\n",
    "    print(f\"  Training: {len(train_products)} products\")\n",
    "    print(f\"  Validation: {len(val_products)} products\")\n",
    "    \n",
    "    # Create datasets with mixed examples\n",
    "    train_dataset = CatProductDataset(\n",
    "        train_products, conversation_examples, explanation_examples, classifier.tokenizer\n",
    "    )\n",
    "    val_dataset = CatProductDataset(\n",
    "        val_products, [], [], classifier.tokenizer  # Val only needs classification\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./models/gemma-cat-lora\",\n",
    "#         use_cpu=True,     # forces Trainer to keep model on CPU\n",
    "#         no_cuda=True,     # (redundant with use_cpu=True but safe)\n",
    "#         use_mps_device=False, #stable on Mac\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        warmup_steps=50,              # Reduced from 100\n",
    "        learning_rate=1e-5,           # KEY CHANGE: Was 3e-4 (6x lower!)\n",
    "        fp16=False,\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        report_to=None\n",
    "    )\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=classifier.model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=classifier.tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Train!\n",
    "    print(\"\\nüöÄ Starting training...\")\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    trainer.save_model(\"./models/gemma-cat-lora\")\n",
    "    print(\"‚úÖ Model saved to ./models/gemma-cat-lora\")\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nüìä Evaluating on validation set...\")\n",
    "    eval_result = trainer.evaluate()\n",
    "    print(f\"  Validation loss: {eval_result['eval_loss']:.4f}\")\n",
    "    \n",
    "    # Test accuracy\n",
    "    correct = 0\n",
    "    for product in val_products[:50]:\n",
    "        pred = classifier.classify(product['name'])\n",
    "        if pred == product['cat_category']:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / 50\n",
    "    print(f\"  Classification accuracy: {accuracy:.2%}\")\n",
    "    \n",
    "    return trainer, train_result, eval_result, val_products\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcab880c",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer, train_result, eval_result, val_products = train_supervised_model(\n",
    "    classifier, cat_products, conversation_examples, explanation_examples, epochs=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b45e62",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Why we validate on classification-only (Optional Note)\n",
    "\n",
    "- __Align eval with the target task__: The end goal is to classify products. By building the validation set in `train_supervised_model()` using `CatProductDataset(val_products, [], [], tokenizer)`, the `eval_loss` (and your post-hoc accuracy) measure what you actually care about.\n",
    "\n",
    "- __Simplicity and speed__: A leaner `val_dataset` lowers eval time and keeps model selection straightforward (select by `eval_loss` on the classification-style sequences).\n",
    "\n",
    "- __Clear interpretability__: When you later print top-line accuracy using `classifier.classify()`, it matches the validation distribution‚Äîno mismatch between how you evaluate during training and how you report after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c07443f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Stage 1: Initial Performance (2 epochs)\n",
    "print(\"\\nüìä Stage 1: After 2 epochs\")\n",
    "print(\"-\"*40)\n",
    "acc1, testres1, resp1 = evaluate_model_capabilities(classifier, val_products)\n",
    "display_model_evaluation(acc1, testres1, resp1)\n",
    "\n",
    "from copy import deepcopy\n",
    "classifier_stage1 = deepcopy(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a73ad40",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Look at the improvement! The model has learned to categorize products like a cat. This is supervised learning: we provided labeled examples, and the model learned the patterns. The mixed training data helps maintain conversational abilities while learning classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7200f609",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Stage 2: Extended Training (2 more epochs)\n",
    "print(\"\\nüìä Stage 2: Training 2 more epochs...\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Update the total epochs to 4 (original 2 + additional 2)\n",
    "trainer.args.num_train_epochs = 4  # This is the key line!\n",
    "\n",
    "# Now continue training - it will train from epoch 2 to epoch 4\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "trainer.save_model(\"./models/gemma-cat-lora-stage2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900d352b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "acc2, testres2, resp2 = evaluate_model_capabilities(classifier, val_products)\n",
    "display_model_evaluation(acc2, testres2, resp2)\n",
    "\n",
    "classifier_stage2 = deepcopy(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc6fa2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Stage 3: Extended Training (2 more epochs)\n",
    "print(\"\\nüìä Stage 3: Training 2 more epochs...\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Update the total epochs to 6 (original 4 + additional 2)\n",
    "trainer.args.num_train_epochs = 6  # This is the key line!\n",
    "\n",
    "# Now continue training - it will train from epoch 4 to epoch 6\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "trainer.save_model(\"./models/gemma-cat-lora-stage3\")\n",
    "\n",
    "acc3, testres3, resp3 = evaluate_model_capabilities(classifier, val_products)\n",
    "display_model_evaluation(acc3, testres3, resp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bab4b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ü§î Let's Analyze Our Results\n",
    "\n",
    "### What do you notice about the progression?\n",
    "\n",
    "#### üí≠ Discussion Questions:\n",
    "1. Why might the model be getting **better** at classification but **worse** at conversation?\n",
    "2. What happened to the model's original language abilities?\n",
    "3. Is more training always better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ef212",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Catastrophic Forgetting: A Core ML Challenge\n",
    "\n",
    "### What's Happening?\n",
    "\n",
    "We're witnessing **catastrophic forgetting** - when a neural network forgets previously learned information while learning new tasks.\n",
    "\n",
    "### Why Does This Happen?\n",
    "\n",
    "1. **Imbalanced Training Data**\n",
    "   - 800 classification examples\n",
    "   - Only 266 conversation examples  \n",
    "   - Classification dominates the learning signal!\n",
    "\n",
    "2. **Single-Task Validation**\n",
    "   - Our validation set only tests classification\n",
    "   - Model optimizes for what we measure\n",
    "   - Conversation quality isn't being tracked\n",
    "\n",
    "3. **Model Capacity Limits**\n",
    "   - Gemma-270M is tiny (only 270 million parameters)\n",
    "   - Must choose: Be good at classification OR conversation\n",
    "   - Not enough \"brain space\" for both!\n",
    "\n",
    "4. **Training Dynamics**\n",
    "   - Later epochs overwrite earlier learning\n",
    "   - High learning rate (3e-4) causes aggressive updates\n",
    "   - No mechanism to preserve original capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e623e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Strategies to Experiment With:\n",
    "\n",
    "#### 1. **Data Balance** (Easiest)\n",
    "```python\n",
    "# Instead of 800:266:242, try 1:1:1 ratio\n",
    "train_dataset = CatProductDataset(\n",
    "    products, conversations*3, explanations*3,  # Triple conversation data\n",
    "    mix_ratio=(1, 1, 1)\n",
    ")\n",
    "```\n",
    "\n",
    "#### 2. **Progressive Learning Rates** (Recommended)\n",
    "```python\n",
    "# Start high, go lower\n",
    "Stage 1: learning_rate=3e-4  # Learn new task\n",
    "Stage 2: learning_rate=1e-4  # Refine\n",
    "Stage 3: learning_rate=5e-5  # Polish (preserve knowledge)\n",
    "```\n",
    "\n",
    "#### 3. **Mixed Validation** \n",
    "```python\n",
    "# Include ALL capabilities in validation\n",
    "val_dataset = CatProductDataset(\n",
    "    val_products, val_conversations, val_explanations\n",
    ")\n",
    "```\n",
    "\n",
    "#### 4. **Architectural Solutions**\n",
    "- **LoRA Rank**: Try `r=16` or `r=32` (more capacity)\n",
    "\n",
    "\n",
    "### Challenge Questions:\n",
    "1. Can you achieve 60% accuracy WITHOUT losing conversation ability?\n",
    "2. What's the minimum model size needed for both tasks?\n",
    "3. How would you design a curriculum to teach both skills?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c369e0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 6: Unsupervised Learning - Discovering Natural Structure\n",
    "\n",
    "Now let's explore **Unsupervised Learning**. We'll see how products naturally cluster without using any labels, and how fine-tuning changes this structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237cdb72",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def true_unsupervised_analysis(embeddings, name=\"Dataset\", k_min=2, k_max=14):\n",
    "    \"\"\"\n",
    "    Perform truly unsupervised clustering analysis.\n",
    "    Find optimal number of clusters without using ground truth labels.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüîç Analyzing {name} (Truly Unsupervised)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Try different numbers of clusters\n",
    "    silhouette_scores = []\n",
    "    davies_bouldin_scores = []\n",
    "    inertias = []\n",
    "    k_range = range(k_min, k_max + 1)\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        clusters = kmeans.fit_predict(embeddings)\n",
    "        \n",
    "        # Silhouette score (higher is better)\n",
    "        sil_score = silhouette_score(embeddings, clusters)\n",
    "        silhouette_scores.append(sil_score)\n",
    "        \n",
    "        # Davies-Bouldin score (lower is better)\n",
    "        db_score = davies_bouldin_score(embeddings, clusters)\n",
    "        davies_bouldin_scores.append(db_score)\n",
    "        \n",
    "        # Inertia (for elbow method)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Find optimal k using silhouette score\n",
    "    optimal_k = silhouette_scores.index(max(silhouette_scores)) + k_min\n",
    "    \n",
    "    print(f\"  Optimal clusters (by Silhouette): {optimal_k}\")\n",
    "    print(f\"  Best Silhouette score: {max(silhouette_scores):.3f}\")\n",
    "    \n",
    "    return optimal_k, silhouette_scores, davies_bouldin_scores, inertias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6ef92",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def compare_embeddings_unsupervised(base_classifier, trained_classifier, products, k_vis=None):\n",
    "    \"\"\"\n",
    "    Compare clustering behavior before and after fine-tuning.\n",
    "    If k_vis is provided, use that k for the visualization clustering\n",
    "    instead of silhouette-optimal k.\n",
    "    \"\"\"\n",
    "    print(\"\\nüîÆ UNSUPERVISED LEARNING: Natural Clustering Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Select subset of products\n",
    "    sample_products = products[:200]\n",
    "    texts = [p['name'] for p in sample_products]\n",
    "    \n",
    "    # Get embeddings from both models\n",
    "    print(\"üìç Extracting embeddings from base model...\")\n",
    "    base_embeddings = base_classifier.get_embeddings(texts)\n",
    "    \n",
    "    print(\"üìç Extracting embeddings from fine-tuned model...\")\n",
    "    trained_embeddings = trained_classifier.get_embeddings(texts)\n",
    "    \n",
    "    # Analyze both without using labels\n",
    "    optimal_k_base, sil_base, db_base, inertia_base = true_unsupervised_analysis(\n",
    "        base_embeddings, \"Base Model\"\n",
    "    )\n",
    "    optimal_k_trained, sil_trained, db_trained, inertia_trained = true_unsupervised_analysis(\n",
    "        trained_embeddings, \"Fine-tuned Model\"\n",
    "    )\n",
    "    \n",
    "    # Visualize the metrics\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    k_range = range(2, 15)\n",
    "    \n",
    "    # Silhouette scores\n",
    "    axes[0, 0].plot(k_range, sil_base, 'b-o', label='Base Model', linewidth=2)\n",
    "    axes[0, 0].plot(k_range, sil_trained, 'r-s', label='Fine-tuned', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Number of Clusters')\n",
    "    axes[0, 0].set_ylabel('Silhouette Score')\n",
    "    axes[0, 0].set_title('Silhouette Analysis (Higher is Better)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].axvline(x=optimal_k_base, color='b', linestyle='--', alpha=0.5)\n",
    "    axes[0, 0].axvline(x=optimal_k_trained, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Davies-Bouldin scores\n",
    "    axes[0, 1].plot(k_range, db_base, 'b-o', label='Base Model', linewidth=2)\n",
    "    axes[0, 1].plot(k_range, db_trained, 'r-s', label='Fine-tuned', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Number of Clusters')\n",
    "    axes[0, 1].set_ylabel('Davies-Bouldin Score')\n",
    "    axes[0, 1].set_title('Davies-Bouldin Analysis (Lower is Better)')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Elbow method\n",
    "    axes[0, 2].plot(k_range, inertia_base, 'b-o', label='Base Model', linewidth=2)\n",
    "    axes[0, 2].plot(k_range, inertia_trained, 'r-s', label='Fine-tuned', linewidth=2)\n",
    "    axes[0, 2].set_xlabel('Number of Clusters')\n",
    "    axes[0, 2].set_ylabel('Inertia')\n",
    "    axes[0, 2].set_title('Elbow Method')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Choose k to cluster for visualization\n",
    "    k_vis_base = k_vis if k_vis is not None else optimal_k_base\n",
    "    k_vis_trained = k_vis if k_vis is not None else optimal_k_trained\n",
    "    \n",
    "    # Cluster with chosen k and visualize\n",
    "    kmeans_base = KMeans(n_clusters=k_vis_base, random_state=42, n_init=10)\n",
    "    kmeans_trained = KMeans(n_clusters=k_vis_trained, random_state=42, n_init=10)\n",
    "    \n",
    "    clusters_base = kmeans_base.fit_predict(base_embeddings)\n",
    "    clusters_trained = kmeans_trained.fit_predict(trained_embeddings)\n",
    "    \n",
    "    # PCA for visualization\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    vis_base = pca.fit_transform(base_embeddings)\n",
    "    vis_trained = pca.fit_transform(trained_embeddings)\n",
    "    \n",
    "    # Plot base model clusters\n",
    "    scatter1 = axes[1, 0].scatter(vis_base[:, 0], vis_base[:, 1], \n",
    "                                  c=clusters_base, cmap='viridis', alpha=0.6)\n",
    "    axes[1, 0].set_title(f'Base Model\\n({k_vis_base} clusters)', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('PCA Component 1')\n",
    "    axes[1, 0].set_ylabel('PCA Component 2')\n",
    "    plt.colorbar(scatter1, ax=axes[1, 0])\n",
    "    \n",
    "    # Plot trained model clusters\n",
    "    scatter2 = axes[1, 1].scatter(vis_trained[:, 0], vis_trained[:, 1],\n",
    "                                  c=clusters_trained, cmap='viridis', alpha=0.6)\n",
    "    axes[1, 1].set_title(f'Fine-tuned Model\\n({k_vis_trained} clusters)', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('PCA Component 1')\n",
    "    axes[1, 1].set_ylabel('PCA Component 2')\n",
    "    plt.colorbar(scatter2, ax=axes[1, 1])\n",
    "    \n",
    "    # Compare cluster characteristics\n",
    "    axes[1, 2].axis('off')\n",
    "    comparison_text = f\"\"\"\n",
    "    Clustering Comparison:\n",
    "    \n",
    "    Base Model:\n",
    "    - Optimal clusters: {optimal_k_base}\n",
    "    - Best Silhouette: {max(sil_base):.3f}\n",
    "    - Natural grouping based on\n",
    "      general language patterns\n",
    "    \n",
    "    Fine-tuned Model:\n",
    "    - Optimal clusters: {optimal_k_trained}\n",
    "    - Best Silhouette: {max(sil_trained):.3f}\n",
    "    - Grouping influenced by\n",
    "      cat-perspective training\n",
    "    \n",
    "    Key Insight:\n",
    "    Fine-tuning reorganizes the\n",
    "    embedding space to reflect\n",
    "    the task-specific structure!\n",
    "    \"\"\"\n",
    "    axes[1, 2].text(0.1, 0.5, comparison_text, fontsize=10, verticalalignment='center')\n",
    "    \n",
    "    plt.suptitle('Unsupervised Learning: How Fine-tuning Changes Natural Clustering', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze what's in each cluster (without using labels)\n",
    "    print(\"\\nüìä Cluster Sample Analysis (Fine-tuned Model):\")\n",
    "    for i in range(min(5, k_vis_trained)):  # Show first 5 clusters\n",
    "        cluster_indices = np.where(clusters_trained == i)[0][:3]  # First 3 items\n",
    "        print(f\"\\n  Cluster {i} samples:\")\n",
    "        for idx in cluster_indices:\n",
    "            print(f\"    - {texts[idx][:50]}...\")\n",
    "    \n",
    "    return optimal_k_base, optimal_k_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af2839",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_cluster_purity(embeddings, clusters, true_labels, label_names, model_name=\"Model\", k=7):\n",
    "    \"\"\"Analyze how pure clusters are with respect to true labels\"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    print(f\"\\nüìä {model_name} Cluster Analysis (k={k})\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    cluster_purities = []\n",
    "    \n",
    "    for cluster_id in range(k):\n",
    "        # Get items in this cluster\n",
    "        cluster_mask = clusters == cluster_id\n",
    "        cluster_labels = [true_labels[i] for i, m in enumerate(cluster_mask) if m]\n",
    "        \n",
    "        if not cluster_labels:\n",
    "            continue\n",
    "            \n",
    "        # Find most common label and its percentage\n",
    "        label_counts = Counter(cluster_labels)\n",
    "        most_common_label, count = label_counts.most_common(1)[0]\n",
    "        purity = count / len(cluster_labels)\n",
    "        cluster_purities.append(purity)\n",
    "        \n",
    "        # Show cluster composition (top 3 categories)\n",
    "        print(f\"\\nCluster {cluster_id} ({len(cluster_labels)} items, {purity:.1%} pure):\")\n",
    "        for label, cnt in label_counts.most_common(3):\n",
    "            pct = cnt / len(cluster_labels) * 100\n",
    "            bar = '‚ñà' * int(pct / 10) + '‚ñë' * (10 - int(pct / 10))\n",
    "            name = label_names[label] if isinstance(label_names, dict) else str(label)\n",
    "            print(f\"  {name:12s} [{bar}] {pct:.0f}%\")\n",
    "    \n",
    "    avg_purity = sum(cluster_purities) / len(cluster_purities) if cluster_purities else 0.0\n",
    "    print(f\"\\n‚û§ Average Purity: {avg_purity:.1%}\")\n",
    "    return avg_purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f133eb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def compare_clustering_perspectives(base_embeddings, trained_embeddings, products, k=4):\n",
    "    \"\"\"Show how base model clusters by human logic vs fine-tuned by cat logic\"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    import numpy as np\n",
    "    \n",
    "    print(\"\\nüîç CLUSTERING PERSPECTIVE COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # True labels\n",
    "    true_cat_labels = [p['cat_category_id'] for p in products]\n",
    "    \n",
    "    # Correct ID -> name mapping\n",
    "    # Prefer ID_TO_CATEGORY if defined; otherwise derive from CATEGORY_TO_ID\n",
    "    try:\n",
    "        label_names = ID_TO_CATEGORY\n",
    "    except NameError:\n",
    "        # Fallback if ID_TO_CATEGORY not defined\n",
    "        label_names = {v: k for k, v in CATEGORY_TO_ID.items()}\n",
    "    \n",
    "    # Cluster both with k\n",
    "    kmeans_base = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_trained = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    \n",
    "    clusters_base = kmeans_base.fit_predict(base_embeddings)\n",
    "    clusters_trained = kmeans_trained.fit_predict(trained_embeddings)\n",
    "    \n",
    "    # Analyze purity\n",
    "    purity_base = analyze_cluster_purity(\n",
    "        base_embeddings, clusters_base, true_cat_labels, \n",
    "        label_names, \"Base Model\", k=k\n",
    "    )\n",
    "    \n",
    "    purity_trained = analyze_cluster_purity(\n",
    "        trained_embeddings, clusters_trained, true_cat_labels,\n",
    "        label_names, \"Fine-tuned Model\", k=k\n",
    "    )\n",
    "    \n",
    "    # Show specific examples to illustrate the difference\n",
    "    print(\"\\nüéØ EXAMPLE: What's in Cluster 0?\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Get sample products from cluster 0 for both models\n",
    "    base_cluster0_idx = np.where(clusters_base == 0)[0][:5]\n",
    "    trained_cluster0_idx = np.where(clusters_trained == 0)[0][:5]\n",
    "    \n",
    "    print(\"Base Model Cluster 0:\")\n",
    "    for idx in base_cluster0_idx:\n",
    "        print(f\"  ‚Ä¢ {products[idx]['name'][:40]:40s} [{products[idx]['cat_category']}]\")\n",
    "    \n",
    "    print(\"\\nFine-tuned Cluster 0:\")\n",
    "    for idx in trained_cluster0_idx:\n",
    "        print(f\"  ‚Ä¢ {products[idx]['name'][:40]:40s} [{products[idx]['cat_category']}]\")\n",
    "    \n",
    "    \n",
    "    return purity_base, purity_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca3725",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_cluster_composition(base_embeddings, trained_embeddings, products, k=4):\n",
    "    \"\"\"Create a heatmap showing how categories distribute across clusters\"\"\"\n",
    "    from sklearn.cluster import KMeans\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Cluster with k\n",
    "    kmeans_base = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans_trained = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    \n",
    "    clusters_base = kmeans_base.fit_predict(base_embeddings)\n",
    "    clusters_trained = kmeans_trained.fit_predict(trained_embeddings)\n",
    "    \n",
    "    # Correct label names\n",
    "    try:\n",
    "        label_names = ID_TO_CATEGORY\n",
    "    except NameError:\n",
    "        label_names = {v: k for k, v in CATEGORY_TO_ID.items()}\n",
    "    \n",
    "    # Create composition matrices\n",
    "    def get_composition_matrix(clusters, true_labels, k):\n",
    "        matrix = np.zeros((k, k), dtype=float)  # k clusters x k categories\n",
    "        for cluster_id in range(k):\n",
    "            cluster_mask = clusters == cluster_id\n",
    "            cluster_items = [true_labels[i] for i, m in enumerate(cluster_mask) if m]\n",
    "            for cat_id in cluster_items:\n",
    "                if 0 <= cat_id < k:\n",
    "                    matrix[cluster_id, cat_id] += 1.0\n",
    "        # Normalize by row (each cluster sums to 1)\n",
    "        row_sums = matrix.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1.0  # avoid divide-by-zero\n",
    "        return matrix / row_sums\n",
    "    \n",
    "    true_labels = [p['cat_category_id'] for p in products]\n",
    "    base_matrix = get_composition_matrix(clusters_base, true_labels, k)\n",
    "    trained_matrix = get_composition_matrix(clusters_trained, true_labels, k)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Base model heatmap\n",
    "    im1 = axes[0].imshow(base_matrix, cmap='YlOrRd', aspect='auto', vmin=0, vmax=0.7)\n",
    "    axes[0].set_title('Base Model\\n(Products cluster by type)', fontweight='bold')\n",
    "    axes[0].set_xlabel('Cat Categories')\n",
    "    axes[0].set_ylabel('Cluster ID')\n",
    "    axes[0].set_xticks(range(k))\n",
    "    axes[0].set_xticklabels([label_names[i] for i in range(k)], rotation=45, ha='right')\n",
    "    axes[0].set_yticks(range(k))\n",
    "    \n",
    "    # Fine-tuned model heatmap\n",
    "    im2 = axes[1].imshow(trained_matrix, cmap='YlOrRd', aspect='auto', vmin=0, vmax=0.7)\n",
    "    axes[1].set_title('Fine-tuned Model\\n(Products cluster by cat behavior)', fontweight='bold')\n",
    "    axes[1].set_xlabel('Cat Categories')\n",
    "    axes[1].set_ylabel('Cluster ID')\n",
    "    axes[1].set_xticks(range(k))\n",
    "    axes[1].set_xticklabels([label_names[i] for i in range(k)], rotation=45, ha='right')\n",
    "    axes[1].set_yticks(range(k))\n",
    "    \n",
    "    # Add colorbars\n",
    "    plt.colorbar(im1, ax=axes[0], label='Proportion')\n",
    "    plt.colorbar(im2, ax=axes[1], label='Proportion')\n",
    "    \n",
    "    plt.suptitle('Cluster Composition: How Categories Distribute Across Clusters', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìñ How to read: Darker = more concentrated\")\n",
    "    print(\"   Diagonal pattern = perfect clustering by category\")\n",
    "    print(\"   Scattered pattern = mixed clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e98927",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Create a fresh base model for comparison\n",
    "print(\"üîß Loading base model for comparison...\")\n",
    "base_classifier = GemmaCatClassifier(use_lora=False)\n",
    "\n",
    "# Get sample of products for analysis\n",
    "sample_products = cat_products[:200]\n",
    "\n",
    "# Extract embeddings once\n",
    "print(\"\\nüìç Extracting embeddings...\")\n",
    "texts = [p['name'] for p in sample_products]\n",
    "base_embeddings = base_classifier.get_embeddings(texts)\n",
    "\n",
    "# Use the Stage 2 classifier here\n",
    "trained_embeddings = classifier_stage1.get_embeddings(texts)\n",
    "\n",
    "K = 4\n",
    "\n",
    "# Unsupervised comparison (metrics use optimal k, visualization forced to K)\n",
    "optimal_k_base, optimal_k_trained = compare_embeddings_unsupervised(\n",
    "    base_classifier, classifier_stage1, sample_products, k_vis=K\n",
    ")\n",
    "\n",
    "# Purity and composition using Stage 2\n",
    "purity_base, purity_trained = compare_clustering_perspectives(\n",
    "    base_embeddings, trained_embeddings, sample_products, k=K\n",
    ")\n",
    "\n",
    "visualize_cluster_composition(\n",
    "    base_embeddings, trained_embeddings, sample_products, k=K\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4bf8da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What Did We Learn?\n",
    "\n",
    "### The Power of Fine-tuning on Representation Space\n",
    "**Key Insight**:\n",
    "   - Supervised learning doesn't just add a classification layer\n",
    "   - It fundamentally **reorganizes the entire representation space**\n",
    "   - Products that cats see similarly become closer in embedding space\n",
    "   - This is why transfer learning works so well!\n",
    "\n",
    "### Why This Matters\n",
    "- **For ML Practice**: Shows that fine-tuning changes deep representations\n",
    "- **For Applications**: Embeddings from fine-tuned models are task-specific\n",
    "- **For Understanding**: Neural networks learn structured representations, not just decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742b980f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 7: Active Learning - Smart Labeling with Gemma\n",
    "\n",
    "### 7.1 The Big Question\n",
    "\n",
    "### What if the model could tell us what to label next?\n",
    "\n",
    "**The Problem:**\n",
    "- Labeling data is expensive üí∞\n",
    "- Most examples are \"easy\" and redundant\n",
    "- We waste time labeling obvious cases\n",
    "\n",
    "**The Solution: Active Learning**\n",
    "- Let the model identify what confuses it\n",
    "- Focus human effort on the hard cases\n",
    "- Achieve high accuracy with minimal labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8537a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### 7.2 How Active Learning Works\n",
    "\n",
    "### The Core Concept\n",
    "\n",
    "\n",
    "**Traditional Approach:**\n",
    "- Label random data\n",
    "- Train model\n",
    "- Hope for the best\n",
    "\n",
    "**Active Learning Approach:**\n",
    "- Start with tiny labeled set\n",
    "- Model identifies confusing examples\n",
    "- Human labels only those\n",
    "- Repeat until accurate\n",
    "\n",
    "\n",
    "**Key Insight:** The model knows what it doesn't know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6c9ee1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "### 7.3 Setting Up the Demo\n",
    "\n",
    "Let's load our pre-computed results and see active learning in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c41d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "# Check if we have pre-computed results\n",
    "CHECKPOINT_DIR = Path('./models/active_learning_checkpoints')\n",
    "ASSETS_DIR = Path('./models/lecture_assets')\n",
    "\n",
    "if CHECKPOINT_DIR.exists():\n",
    "    print(\"‚úÖ Pre-computed checkpoints found!\")\n",
    "    print(f\"   Found {len(list(CHECKPOINT_DIR.glob('checkpoint_*')))} checkpoints\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No checkpoints found. Please run: python prepare_lecture_demo.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba5fd20",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 7.4 Live Uncertainty Demo\n",
    "\n",
    "### See How the Model Identifies Confusion\n",
    "\n",
    "Let's start with a live demonstration of how the model calculates uncertainty:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d865880",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def live_uncertainty_demo(classifier, products=None):\n",
    "    \"\"\"\n",
    "    Interactive demo showing how the model identifies confusing products\n",
    "    Perfect for explaining the concept!\n",
    "    \"\"\"\n",
    "    print(\"üîç LIVE DEMO: How Active Learning Identifies Confusion\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Test products that showcase different uncertainty levels\n",
    "    demo_products = [\n",
    "        (\"laptop computer\", \"Ambiguous: warm surface or electronics?\"),\n",
    "        (\"cardboard box\", \"Classic cat item, but play or sleep?\"),\n",
    "        (\"laser pointer\", \"Clearly a toy... or is it?\"),\n",
    "        (\"vacuum cleaner\", \"Definitely scary!\"),\n",
    "        (\"cat food bowl\", \"Obviously for eating\"),\n",
    "        (\"bluetooth speaker\", \"Toy-like but not really\"),\n",
    "        (\"electric blanket\", \"Warm but also dangerous?\"),\n",
    "        (\"paper bag\", \"Territory or play?\"),\n",
    "    ]\n",
    "    \n",
    "    uncertainties = []\n",
    "    \n",
    "    print(\"\\nCalculating uncertainty for each product:\\n\")\n",
    "    \n",
    "    for product, description in demo_products:\n",
    "        # Get prediction and probabilities\n",
    "        pred, probs = classifier.classify(product, return_probs=True)\n",
    "        \n",
    "        # Calculate entropy (uncertainty)\n",
    "        entropy = -sum(p * np.log(p + 1e-10) for p in probs.values() if p > 0)\n",
    "        \n",
    "        # Visual uncertainty meter\n",
    "        bar_length = int(entropy * 10)\n",
    "        uncertainty_bar = '‚ñà' * bar_length + '‚ñë' * (10 - bar_length)\n",
    "        \n",
    "        # Confidence of top prediction\n",
    "        confidence = probs[pred]\n",
    "        \n",
    "        # Get top 2 categories for confusion analysis\n",
    "        sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:2]\n",
    "        \n",
    "        print(f\"üì¶ {product:20s}\")\n",
    "        print(f\"   {description}\")\n",
    "        print(f\"   Prediction: {pred:12s} (confidence: {confidence:.1%})\")\n",
    "        print(f\"   Uncertainty: [{uncertainty_bar}] {entropy:.3f}\")\n",
    "        \n",
    "        if entropy > 1.0:  # High uncertainty\n",
    "            print(f\"   ‚ö†Ô∏è CONFUSED between {sorted_probs[0][0]} ({sorted_probs[0][1]:.1%}) \"\n",
    "                  f\"and {sorted_probs[1][0]} ({sorted_probs[1][1]:.1%})\")\n",
    "            print(f\"   ‚úÖ PERFECT for active learning!\\n\")\n",
    "        else:\n",
    "            print(f\"   ‚úì Pretty confident, lower priority\\n\")\n",
    "        \n",
    "        uncertainties.append((product, entropy, pred, confidence))\n",
    "    \n",
    "    # Sort by uncertainty\n",
    "    uncertainties.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"üéØ ACTIVE LEARNING WOULD SELECT:\")\n",
    "    print(f\"   ‚Üí '{uncertainties[0][0]}' (uncertainty: {uncertainties[0][1]:.3f})\")\n",
    "    print(\"\\nüí° This is the product that would teach the model the most!\")\n",
    "    \n",
    "    return uncertainties\n",
    "\n",
    "# Run the demo with our trained classifier\n",
    "uncertainty_results = live_uncertainty_demo(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95c57c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### 7.5 The Power of Smart Selection\n",
    "\n",
    "### Watch How Active Learning Outperforms Random Sampling\n",
    "\n",
    "Now let's load our pre-computed results and see the dramatic difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc2ad9d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Load results\n",
    "CHECKPOINT_DIR = Path(\"models/active_learning_checkpoints\")\n",
    "RESULTS_PATH = CHECKPOINT_DIR / \"results.json\"\n",
    "with open(RESULTS_PATH, \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(\"üìä ACTIVE LEARNING VS RANDOM SAMPLING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "active = results[\"active_learning\"]\n",
    "random_ = results[\"random_sampling\"]\n",
    "\n",
    "initial = results.get(\"initial_samples\", 7)\n",
    "step = 5\n",
    "\n",
    "def x_axis(accs): \n",
    "    return [initial + i*step for i in range(len(accs))]\n",
    "\n",
    "a_acc = active.get(\"accuracies\", [])\n",
    "r_acc = random_.get(\"accuracies\", [])\n",
    "xa, xr = x_axis(a_acc), x_axis(r_acc)\n",
    "\n",
    "# 1) Equal-budget comparison (use last common label count)\n",
    "common_budget = min(xa[-1] if xa else 0, xr[-1] if xr else 0)\n",
    "def value_at_budget(xs, ys, budget):\n",
    "    if not xs: return 0.0\n",
    "    # nearest index (xs are monotonic)\n",
    "    idx = min(range(len(xs)), key=lambda i: abs(xs[i]-budget))\n",
    "    return ys[idx]\n",
    "\n",
    "a_eq = value_at_budget(xa, a_acc, common_budget)\n",
    "r_eq = value_at_budget(xr, r_acc, common_budget)\n",
    "delta_pp = (a_eq - r_eq) * 100\n",
    "\n",
    "print(f\"\\nü™ô Equal-budget comparison at {common_budget} labels:\")\n",
    "print(f\"   Active Learning: {a_eq:.1%}\")\n",
    "print(f\"   Random Sampling: {r_eq:.1%}\")\n",
    "print(f\"   Advantage: {delta_pp:.1f} percentage points\")\n",
    "\n",
    "# 2) Label-efficiency: AUC of accuracy vs labels up to common budget\n",
    "def auc(xs, ys, limit):\n",
    "    if not xs: return 0.0\n",
    "    # truncate to <= limit\n",
    "    x_t, y_t = zip(*[(x, y) for x, y in zip(xs, ys) if x <= limit]) if xs[0] <= limit else ([], [])\n",
    "    if len(x_t) < 2: return 0.0\n",
    "    return np.trapz(y_t, x_t)\n",
    "\n",
    "auc_a = auc(xa, a_acc, common_budget)\n",
    "auc_r = auc(xr, r_acc, common_budget)\n",
    "auc_gain = (auc_a - auc_r) / max(auc_r, 1e-8) * 100\n",
    "\n",
    "print(f\"\\nüìê Label-efficiency (AUC up to {common_budget} labels):\")\n",
    "print(f\"   Active: {auc_a:.3f} | Random: {auc_r:.3f} | Gain: {auc_gain:.1f}%\")\n",
    "\n",
    "# 3) Early gain (after first retrain milestone, e.g., +5 labels)\n",
    "milestone = initial + 5\n",
    "a_m = value_at_budget(xa, a_acc, milestone)\n",
    "r_m = value_at_budget(xr, r_acc, milestone)\n",
    "print(f\"\\nüöÄ Early gain at {milestone} labels:\")\n",
    "print(f\"   Active: {a_m:.1%} vs Random: {r_m:.1%} | Œî={((a_m-r_m)*100):.1f} pp\")\n",
    "\n",
    "# 4) Confidence/coverage deltas (if available)\n",
    "a_conf = active.get(\"avg_confidences\", [])\n",
    "r_conf = random_.get(\"avg_confidences\", [])\n",
    "if a_conf and r_conf:\n",
    "    a_conf_eq = value_at_budget(x_axis(a_conf), a_conf, common_budget)\n",
    "    r_conf_eq = value_at_budget(x_axis(r_conf), r_conf, common_budget)\n",
    "    print(f\"\\nüîí Confidence at {common_budget} labels:\")\n",
    "    print(f\"   Active: {a_conf_eq:.1%} vs Random: {r_conf_eq:.1%}\")\n",
    "\n",
    "a_cov = active.get(\"category_coverage\", [])\n",
    "r_cov = random_.get(\"category_coverage\", [])\n",
    "if a_cov and r_cov:\n",
    "    a_cov_eq = value_at_budget(x_axis(a_cov), a_cov, common_budget)\n",
    "    r_cov_eq = value_at_budget(x_axis(r_cov), r_cov, common_budget)\n",
    "    print(f\"\\nüß≠ Category coverage at {common_budget} labels:\")\n",
    "    print(f\"   Active: {a_cov_eq:.0f} vs Random: {r_cov_eq:.0f} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b53f40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 7.6 Visualizing the Advantage\n",
    "\n",
    "### See the Dramatic Difference in Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df3a2c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_active_vs_random(results):\n",
    "    active = results[\"active_learning\"]\n",
    "    random_ = results[\"random_sampling\"]\n",
    "    initial = results.get(\"initial_samples\", 7)\n",
    "    step = 5\n",
    "\n",
    "    def x_axis(arr): \n",
    "        return [initial + i*step for i in range(len(arr))]\n",
    "\n",
    "    a_acc = active.get(\"accuracies\", [])\n",
    "    r_acc = random_.get(\"accuracies\", [])\n",
    "    xa, xr = x_axis(a_acc), x_axis(r_acc)\n",
    "\n",
    "    # Confidence x-axes\n",
    "    a_conf = active.get(\"avg_confidences\", [])\n",
    "    r_conf = random_.get(\"avg_confidences\", [])\n",
    "    xa_conf, xr_conf = x_axis(a_conf), x_axis(r_conf)\n",
    "\n",
    "    # Common budget and helpers\n",
    "    common_budget = min(xa[-1] if xa else 0, xr[-1] if xr else 0)\n",
    "    def val_at(xs, ys, x0):\n",
    "        if not xs: return 0.0\n",
    "        idx = min(range(len(xs)), key=lambda i: abs(xs[i]-x0))\n",
    "        return ys[idx]\n",
    "\n",
    "    a_eq = val_at(xa, a_acc, common_budget)\n",
    "    r_eq = val_at(xr, r_acc, common_budget)\n",
    "\n",
    "    def auc(xs, ys, limit):\n",
    "        if not xs: return 0.0\n",
    "        pts = [(x, y) for x, y in zip(xs, ys) if x <= limit]\n",
    "        if len(pts) < 2: return 0.0\n",
    "        x_t, y_t = zip(*pts)\n",
    "        return np.trapz(y_t, x_t)\n",
    "\n",
    "    auc_a = auc(xa, a_acc, common_budget)\n",
    "    auc_r = auc(xr, r_acc, common_budget)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # 1) Accuracy curves + equal-budget marker\n",
    "    ax1.plot(xa, [y*100 for y in a_acc], 'b-o', label='Active', linewidth=3, markersize=7)\n",
    "    ax1.plot(xr, [y*100 for y in r_acc], 'r--s', label='Random', linewidth=2, markersize=6, alpha=0.8)\n",
    "    if common_budget:\n",
    "        ax1.axvline(common_budget, color='gray', linestyle=':', alpha=0.6)\n",
    "        ax1.annotate(f'Equal budget: {common_budget}', xy=(common_budget, 0.5), xytext=(common_budget+2, 50),\n",
    "                     arrowprops=dict(arrowstyle='->', color='gray'), fontsize=10, color='gray')\n",
    "\n",
    "        # Delta label at budget\n",
    "        ax1.text(common_budget+1, (a_eq*100 + r_eq*100)/2, f'+{(a_eq-r_eq)*100:.1f} pp', color='blue', fontsize=10)\n",
    "\n",
    "    ax1.set_xlabel('Number of Labeled Examples', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylabel('Accuracy (%)', fontsize=13, fontweight='bold')\n",
    "    ax1.set_title('Active vs Random at Same Label Budget', fontsize=15, fontweight='bold')\n",
    "    ax1.legend(fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    # Auto-scale to your observed range\n",
    "    all_acc = [*(y*100 for y in a_acc), *(y*100 for y in r_acc)]\n",
    "    if all_acc:\n",
    "        lo = max(0, min(all_acc) - 5); hi = min(100, max(all_acc) + 5)\n",
    "        ax1.set_ylim([lo, hi])\n",
    "\n",
    "    # 2) Confidence evolution (if present)\n",
    "    if a_conf or r_conf:\n",
    "        if a_conf:\n",
    "            ax2.plot(xa_conf, [c*100 for c in a_conf], 'b-o', label='Active', linewidth=2.5, markersize=6)\n",
    "        if r_conf:\n",
    "            ax2.plot(xr_conf, [c*100 for c in r_conf], 'r--s', label='Random', linewidth=2, markersize=5, alpha=0.8)\n",
    "        ax2.set_xlabel('Number of Labeled Examples', fontsize=12)\n",
    "        ax2.set_ylabel('Confidence (%)', fontsize=12)\n",
    "        ax2.set_title('Confidence Over Labels', fontsize=14, fontweight='bold')\n",
    "        ax2.legend(fontsize=11)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax2.axis('off')\n",
    "        ax2.set_title('Confidence Not Logged', fontsize=14)\n",
    "\n",
    "    # 3) Label-efficiency via AUC up to equal budget\n",
    "    x = np.arange(2)\n",
    "    width = 0.6\n",
    "    bars = ax3.bar(x, [auc_a, auc_r], color=['#2E86AB', '#F18F01'], alpha=0.85, width=width)\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(['Active', 'Random'])\n",
    "    ax3.set_ylabel('AUC (accuracy vs labels)', fontsize=12, fontweight='bold')\n",
    "    ax3.set_title(f'Label Efficiency ‚â§ {common_budget} labels', fontsize=14, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    for bar in bars:\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., bar.get_height() * 1.01,\n",
    "                 f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "    plt.suptitle('Active Learning: Better Accuracy for the Same Label Budget', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_active_vs_random(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d79d43",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### 7.7 Interactive Uncertainty Explorer\n",
    "\n",
    "### Play with Different Products to See Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebecdbc7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def create_interactive_explorer():\n",
    "    \"\"\"Interactive widget to explore uncertainty on any product\"\"\"\n",
    "    \n",
    "    print(\"üéÆ INTERACTIVE UNCERTAINTY EXPLORER\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Try different products to see what confuses the model!\\n\")\n",
    "    \n",
    "    @interact(product_name=widgets.Text(\n",
    "        value='laptop computer',\n",
    "        placeholder='Enter a product name',\n",
    "        description='Product:',\n",
    "        style={'description_width': 'initial'}\n",
    "    ))\n",
    "    def explore_uncertainty(product_name):\n",
    "        if not product_name:\n",
    "            return\n",
    "        \n",
    "        # Get predictions\n",
    "        pred, probs = classifier.classify(product_name, return_probs=True)\n",
    "        \n",
    "        # Calculate entropy\n",
    "        entropy = -sum(p * np.log(p + 1e-10) for p in probs.values() if p > 0)\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Probability distribution\n",
    "        categories = list(probs.keys())\n",
    "        values = list(probs.values())\n",
    "        colors = ['#2E86AB' if v == max(values) else '#CCCCCC' for v in values]\n",
    "        \n",
    "        bars = ax1.barh(categories, values, color=colors)\n",
    "        ax1.set_xlabel('Probability', fontsize=12)\n",
    "        ax1.set_title(f'Model Predictions for \"{product_name}\"', fontsize=13, fontweight='bold')\n",
    "        ax1.set_xlim(0, 1)\n",
    "        \n",
    "        # Add percentage labels\n",
    "        for bar, val in zip(bars, values):\n",
    "            ax1.text(val + 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                    f'{val:.1%}', va='center', fontsize=10)\n",
    "        \n",
    "        # Uncertainty meter\n",
    "        ax2.clear()\n",
    "        \n",
    "        # Create uncertainty gauge\n",
    "        if entropy > 1.5:\n",
    "            color = '#FF4444'\n",
    "            label = \"HIGH\\nPRIORITY!\"\n",
    "            message = \"Perfect for labeling!\"\n",
    "        elif entropy > 1.0:\n",
    "            color = '#FFA500'\n",
    "            label = \"Medium\\nPriority\"\n",
    "            message = \"Good candidate\"\n",
    "        else:\n",
    "            color = '#44AA44'\n",
    "            label = \"Low\\nPriority\"\n",
    "            message = \"Model is confident\"\n",
    "        \n",
    "        # Draw gauge\n",
    "        bar = ax2.bar(['Uncertainty'], [entropy], color=color, width=0.5)\n",
    "        ax2.set_ylim(0, 2.5)\n",
    "        ax2.set_ylabel('Entropy', fontsize=12)\n",
    "        ax2.set_title('Should We Label This?', fontsize=13, fontweight='bold')\n",
    "        \n",
    "        # Add text\n",
    "        ax2.text(0, entropy + 0.1, label, ha='center', fontsize=12, fontweight='bold')\n",
    "        ax2.text(0, -0.2, message, ha='center', fontsize=11, style='italic')\n",
    "        \n",
    "        # Add entropy value\n",
    "        ax2.text(0, entropy/2, f'{entropy:.3f}', ha='center', va='center', \n",
    "                fontsize=14, fontweight='bold', color='white')\n",
    "        \n",
    "        plt.suptitle(f'Prediction: {pred} (Confidence: {probs[pred]:.1%})', \n",
    "                    fontsize=14, fontweight='bold', y=1.05)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print confusion analysis\n",
    "        sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:2]\n",
    "        if entropy > 1.0:\n",
    "            print(f\"\\n‚ö†Ô∏è Model is confused between:\")\n",
    "            print(f\"   ‚Ä¢ {sorted_probs[0][0]}: {sorted_probs[0][1]:.1%}\")\n",
    "            print(f\"   ‚Ä¢ {sorted_probs[1][0]}: {sorted_probs[1][1]:.1%}\")\n",
    "            print(f\"\\n‚úÖ This would be a great example to label!\")\n",
    "        else:\n",
    "            print(f\"\\n‚úì Model is confident: {pred} ({probs[pred]:.1%})\")\n",
    "            print(f\"  Lower priority for labeling\")\n",
    "\n",
    "# Create the interactive explorer\n",
    "create_interactive_explorer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0a1686",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### 7.8 Progressive Model Improvement\n",
    "\n",
    "### Watch the Model Learn with Each Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837bb00e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CHECKPOINT_DIR = Path(\"models/active_learning_checkpoints\")\n",
    "RESULTS_PATH = CHECKPOINT_DIR / \"results.json\"\n",
    "\n",
    "def show_progressive_learning_real(results_path=RESULTS_PATH):\n",
    "    with open(results_path, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    active = results[\"active_learning\"]\n",
    "    random_ = results[\"random_sampling\"]  # Not used here, but available\n",
    "\n",
    "    initial = results.get(\"initial_samples\", 7)\n",
    "    step = 5\n",
    "\n",
    "    def x_axis(arr): \n",
    "        return [initial + i*step for i in range(len(arr))]\n",
    "\n",
    "    a_acc = active.get(\"accuracies\", [])\n",
    "    a_conf = active.get(\"avg_confidences\", [])\n",
    "    a_unc  = active.get(\"uncertainties\", [])\n",
    "    a_cov  = active.get(\"category_coverage\", [])\n",
    "\n",
    "    xa_acc  = x_axis(a_acc)\n",
    "    xa_conf = x_axis(a_conf)\n",
    "    xa_unc  = list(range(len(a_unc)))  # uncertainties are per-round; index is fine\n",
    "    xa_cov  = x_axis(a_cov)\n",
    "\n",
    "    print(\"üìà PROGRESSIVE MODEL IMPROVEMENT (Real Data)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nWatch how the model gets smarter at each evaluation checkpoint:\\n\")\n",
    "\n",
    "    # Build a unified list of checkpoints from accuracy (since eval logged every 5 labels)\n",
    "    checkpoints = xa_acc\n",
    "\n",
    "    # Helper to get value at a checkpoint from a metric with its own x-axis\n",
    "    def val_at(xs, ys, x0, default=None):\n",
    "        if not xs or not ys:\n",
    "            return default\n",
    "        if x0 in xs:\n",
    "            return ys[xs.index(x0)]\n",
    "        # nearest prior\n",
    "        prior = [(x, y) for x, y in zip(xs, ys) if x <= x0]\n",
    "        if prior:\n",
    "            return prior[-1][1]\n",
    "        return ys[0] if ys else default\n",
    "\n",
    "    print(\"N  | Accuracy  | Confidence | Uncertainty | Coverage | Visual\")\n",
    "    print(\"---|-----------|------------|-------------|----------|\" + \"-\" * 35)\n",
    "\n",
    "    for N in checkpoints:\n",
    "        acc = val_at(xa_acc, a_acc, N, 0.0)                # 0‚Äì1\n",
    "        conf = val_at(xa_conf, a_conf, N, None)            # 0‚Äì1 or None\n",
    "        # For uncertainty, map eval checkpoints to nearest selection index proportionally\n",
    "        # If you prefer, show N/A when a strict mapping isn‚Äôt possible.\n",
    "        unc = None\n",
    "        if a_unc:\n",
    "            # Approximate mapping: use proportional index\n",
    "            idx = min(len(a_unc)-1, max(0, round((N - initial) / step) - 1))\n",
    "            unc = a_unc[idx]\n",
    "\n",
    "        cov = val_at(xa_cov, a_cov, N, None)              # 0‚Äì7 or None\n",
    "\n",
    "        # Visual bar for confidence if available\n",
    "        if conf is not None:\n",
    "            conf_bar_length = int(max(0, min(30, round(conf * 30))))\n",
    "            conf_bar = '‚ñà' * conf_bar_length + '‚ñë' * (30 - conf_bar_length)\n",
    "            status = \"‚úÖ\" if conf >= 0.8 else (\"üî∂\" if conf >= 0.6 else \"‚ùå\")\n",
    "            conf_str = f\"{conf:10.1%}\"\n",
    "        else:\n",
    "            conf_bar = '¬∑' * 30\n",
    "            status = \"‚Äì\"\n",
    "            conf_str = f\"{'N/A':>10s}\"\n",
    "\n",
    "        unc_str = f\"{unc:11.3f}\" if unc is not None else f\"{'N/A':>11s}\"\n",
    "        cov_str = f\"{int(round(cov)):2d}/7\" if cov is not None else \"N/A\"\n",
    "\n",
    "        print(f\"{N:2d} | {acc:9.1%} | {conf_str} | {unc_str} | {cov_str:>8s} | [{conf_bar}] {status}\")\n",
    "\n",
    "    print(\"\\n‚ú® See confidence grow, uncertainty trend down, and coverage progress as labels increase.\")\n",
    "\n",
    "    # Plots with real arrays and per-metric x-axes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # 1) Accuracy and Confidence (if present)\n",
    "    ax1.plot(xa_acc, [y*100 for y in a_acc], 'b-o', linewidth=3, markersize=6, label='Accuracy')\n",
    "    if a_conf:\n",
    "        ax1.plot(xa_conf, [c*100 for c in a_conf], 'g--s', linewidth=2, markersize=5, alpha=0.9, label='Confidence')\n",
    "    ax1.set_xlabel('Number of Labeled Examples', fontsize=12)\n",
    "    ax1.set_ylabel('Metric (%)', fontsize=12)\n",
    "    ax1.set_title('Accuracy and Confidence over Labels', fontsize=13, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(loc='lower right')\n",
    "\n",
    "    # 2) Uncertainty and Coverage (if present)\n",
    "    plotted_any = False\n",
    "    if a_unc:\n",
    "        ax2.plot(range(len(a_unc)), a_unc, 'r-s', linewidth=2.5, markersize=6, label='Avg Uncertainty')\n",
    "        plotted_any = True\n",
    "    if a_cov:\n",
    "        ax2_ = ax2.twinx()\n",
    "        ax2_.plot(xa_cov, a_cov, 'k-.^', linewidth=2, markersize=5, label='Category Coverage')\n",
    "        ax2_.set_ylabel('Coverage (categories)', fontsize=12)\n",
    "        ax2_.set_ylim(0, 7.5)\n",
    "        # Merge legends\n",
    "        lines, labels = ax2.get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2_.get_legend_handles_labels()\n",
    "        ax2.legend(lines + lines2, labels + labels2, loc='upper right')\n",
    "        plotted_any = True\n",
    "\n",
    "    ax2.set_xlabel('Round (uncertainty) / Labels (coverage)', fontsize=12)\n",
    "    ax2.set_title('Uncertainty Reduction and Coverage', fontsize=13, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    if not plotted_any:\n",
    "        ax2.axis('off')\n",
    "        ax2.set_title('No Uncertainty/Coverage Logged', fontsize=13)\n",
    "\n",
    "    plt.suptitle('Active Learning: From Confusion to Confidence (Real Data)', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_progressive_learning_real()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e589cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "This is the power of active learning! Instead of randomly labeling data, we let the model guide us to the most informative examples. In production systems, this can reduce labeling costs by 50-70% while achieving the same accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bccdc79",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Comparing ML Paradigms: Supervised, Unsupervised, and Active Learning\n",
    "\n",
    "## What each paradigm is best at\n",
    "- **Supervised Learning**\n",
    "  - Optimizes for maximum predictive accuracy once you have labeled data.\n",
    "  - Reliable and production-ready when labels are abundant and consistent.\n",
    "\n",
    "- **Unsupervised Learning**\n",
    "  - Explores structure in unlabeled data to reveal patterns and groups.\n",
    "  - Great for discovery, sense-making, and informing downstream tasks.\n",
    "\n",
    "- **Active Learning**\n",
    "  - Selects the most informative examples to label next.\n",
    "  - Aims to reach useful performance with far fewer labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c3b244",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "## Trade-offs and costs\n",
    "- **Supervised**\n",
    "  - Strengths: Highest ceiling on accuracy; stable training.\n",
    "  - Costs: Labeling is expensive and time-consuming; up-front investment required.\n",
    "\n",
    "- **Unsupervised**\n",
    "  - Strengths: No labels needed; fast to explore; uncovers hidden structure.\n",
    "  - Costs: Not directly optimizing a labeled objective; requires interpretation.\n",
    "\n",
    "- **Active**\n",
    "  - Strengths: Label-efficient; focuses human effort where it matters; faster early gains.\n",
    "  - Costs: Iterative loop (model+query+label); requires uncertainty/selection strategy.\n",
    "\n",
    "## When to use which\n",
    "- **Supervised:** Production systems, high-stakes decisions, when labels are available and accuracy is paramount.\n",
    "- **Unsupervised:** Early-stage exploration, market/customer segmentation, discovering anomalies or themes.\n",
    "- **Active:** Limited labeling budget, costly experts, need quick improvements with minimal labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8abddfa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Part 8: Testing Our Complete Cat Assistant\n",
    "\n",
    "Let's have some fun and test our trained model's ability to both classify and chat!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f42afe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def interactive_cat_demo(classifier):\n",
    "    \"\"\"\n",
    "    Interactive demo showing both classification and conversation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üê± INTERACTIVE CAT ASSISTANT DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    demo_products = [\n",
    "        \"Apple MacBook Pro laptop\",\n",
    "        \"Large cardboard shipping box\",\n",
    "        \"Automatic laser pointer toy\",\n",
    "        \"Roomba robot vacuum\",\n",
    "        \"Ceramic water fountain\",\n",
    "        \"Fleece blanket\",\n",
    "        \"Bluetooth speaker\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüéØ Classification Mode:\\n\")\n",
    "    for product in demo_products:\n",
    "        pred, probs = classifier.classify(product, return_probs=True)\n",
    "        confidence = probs[pred]\n",
    "        uncertainty = classifier.get_uncertainty(product)\n",
    "        \n",
    "        # Create visual confidence bar\n",
    "        bar_length = int(confidence * 20)\n",
    "        bar = '‚ñà' * bar_length + '‚ñë' * (20 - bar_length)\n",
    "        \n",
    "        print(f\"üì¶ {product:30s}\")\n",
    "        print(f\"   ‚Üí {pred:12s} [{bar}] {confidence:.1%}\")\n",
    "        print(f\"   Uncertainty: {uncertainty:.3f}\")\n",
    "    \n",
    "    print(\"\\nüí¨ Conversation Mode:\\n\")\n",
    "    \n",
    "    # Test conversational ability\n",
    "    conversations = [\n",
    "        \"What would a cat think about a warm laptop?\",\n",
    "        \"My cat keeps sitting on my keyboard. Why?\",\n",
    "        \"Is a cardboard box a good cat toy?\"\n",
    "    ]\n",
    "    \n",
    "    for question in conversations:\n",
    "        prompt = f\"Human: {question}\\nAssistant:\"\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = classifier.tokenizer(prompt, return_tensors=\"pt\").to(classifier.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = classifier.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=50,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=classifier.tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        response = classifier.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = response[len(prompt):].strip()\n",
    "        \n",
    "        print(f\"üë§ Human: {question}\")\n",
    "        print(f\"üê± Cat Assistant: {response}\\n\")\n",
    "    \n",
    "    print(\"‚ú® Notice how the model maintains both capabilities!\")\n",
    "    print(\"   It can classify products AND have conversations about them.\")\n",
    "\n",
    "# Run the demo\n",
    "interactive_cat_demo(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000a651",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Our model can do both classification and conversation! This dual capability is crucial for real-world applications. We didn't sacrifice the model's general abilities to teach it our specific task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c052c17",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 9: Running the CatShop Web Application\n",
    "\n",
    "Now that we've trained our cat classifier, let's see it in action in a real web application! The CatShop website integrates our trained model to provide:\n",
    "\n",
    "1. **Cat Classifications**: Each product shows how a cat would categorize it\n",
    "2. **Confidence Scores**: Visual indicators of how certain the cat is\n",
    "3. **Cat Chat**: Interactive chat with the cat about products\n",
    "\n",
    "Let's set up and run the web application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d2b6fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# First, ensure the trained model is in the expected location for the web app\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# The web app expects the model in a specific location relative to the catshop module\n",
    "web_model_path = Path('catshop/models/gemma-cat-lora')\n",
    "notebook_model_path = Path('models/gemma-cat-lora')\n",
    "\n",
    "# Create the directory structure if it doesn't exist\n",
    "web_model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy the trained model to where the web app expects it\n",
    "if notebook_model_path.exists():\n",
    "    print(f\"‚úÖ Copying trained model from {notebook_model_path} to {web_model_path}\")\n",
    "    if web_model_path.exists():\n",
    "        shutil.rmtree(web_model_path)\n",
    "    shutil.copytree(notebook_model_path, web_model_path)\n",
    "    print(f\"‚úÖ Model copied successfully!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è No trained model found at {notebook_model_path}\")\n",
    "    print(\"   The web app will fall back to rule-based classification\")\n",
    "\n",
    "# Verify the model files are in place\n",
    "if web_model_path.exists():\n",
    "    model_files = list(web_model_path.glob('*'))\n",
    "    print(f\"\\nüìÅ Model files in {web_model_path}:\")\n",
    "    for f in model_files[:5]:  # Show first 5 files\n",
    "        print(f\"   - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74347fa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Lightweight web demo deps (avoid pyserini/nmslib native build issues)\n",
    "import subprocess, sys, os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # quiet HF tokenizers warning\n",
    "\n",
    "required = [\n",
    "    \"flask\",\n",
    "    \"flask-cors\",\n",
    "    \"rank-bm25\",\n",
    "    \"spacy\",\n",
    "    \"thefuzz\",     # fuzzy string matching\n",
    "    \"rich\"\n",
    "]\n",
    "\n",
    "def ensure(pkg, import_name=None, post=None):\n",
    "    import_name = import_name or pkg.replace(\"-\", \"_\")\n",
    "    try:\n",
    "        __import__(import_name)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {pkg}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "    if post:\n",
    "        post()\n",
    "\n",
    "def _ensure_spacy_model():\n",
    "    try:\n",
    "        import spacy\n",
    "        spacy.load(\"en_core_web_sm\")\n",
    "    except Exception:\n",
    "        print(\"Downloading spaCy English model...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
    "\n",
    "for p in required:\n",
    "    ensure(p)\n",
    "\n",
    "_ensure_spacy_model()\n",
    "\n",
    "# Try optional pyserini (will skip if it fails)\n",
    "try:\n",
    "    __import__(\"pyserini\")\n",
    "    print(\"‚úÖ Optional: pyserini available\")\n",
    "except Exception:\n",
    "    print(\"‚ÑπÔ∏è Optional: pyserini not installed (skipping). Using BM25/fuzzy matching instead.\")\n",
    "\n",
    "print(\"‚úÖ All web demo dependencies ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb1ab6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Update the cat_classifier.py to use relative paths correctly\n",
    "cat_classifier_path = Path('catshop/cat_classifier.py')\n",
    "\n",
    "if cat_classifier_path.exists():\n",
    "    # Read the file\n",
    "    with open(cat_classifier_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Update the model path to be relative to the catshop directory\n",
    "    # The original uses Path(__file__).parent.parent which may not work in notebook context\n",
    "    updated_content = content.replace(\n",
    "        'self.model_path = Path(__file__).parent.parent / model_path',\n",
    "        'self.model_path = Path(\"catshop\") / model_path if Path(\"catshop\").exists() else Path(model_path)'\n",
    "    )\n",
    "    \n",
    "    # Write back if changed\n",
    "    if updated_content != content:\n",
    "        with open(cat_classifier_path, 'w') as f:\n",
    "            f.write(updated_content)\n",
    "        print(\"‚úÖ Updated cat_classifier.py paths for notebook compatibility\")\n",
    "    else:\n",
    "        print(\"‚úÖ cat_classifier.py paths already configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b250e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all unused imports from engine.py\n",
    "from pathlib import Path\n",
    "\n",
    "engine_path = Path('catshop/engine/engine.py')\n",
    "\n",
    "if engine_path.exists():\n",
    "    with open(engine_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Remove unused imports\n",
    "    unused_imports = ['import cleantext', 'from selenium']\n",
    "    \n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        skip = False\n",
    "        for unused in unused_imports:\n",
    "            if unused in line:\n",
    "                print(f\"  Removing: {line.strip()}\")\n",
    "                skip = True\n",
    "                break\n",
    "        if not skip:\n",
    "            cleaned_lines.append(line)\n",
    "    \n",
    "    with open(engine_path, 'w') as f:\n",
    "        f.writelines(cleaned_lines)\n",
    "    \n",
    "    print(\"‚úÖ Cleaned up unused imports from engine.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d3c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import os\n",
    "from IPython.display import IFrame, display, HTML\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, str(Path(\"Lecture 1 Overview\").resolve()))\n",
    "# sanity check\n",
    "import importlib; importlib.import_module(\"catshop\")\n",
    "\n",
    "# Set up Flask app in a thread\n",
    "def run_flask_app():\n",
    "    \"\"\"Run the Flask application in a separate thread\"\"\"\n",
    "    import sys\n",
    "    sys.path.insert(0, 'catshop')\n",
    "    \n",
    "    # Change to catshop directory for proper static file serving\n",
    "    original_dir = os.getcwd()\n",
    "    os.chdir('catshop')\n",
    "    \n",
    "    try:\n",
    "        from app import app\n",
    "        # Run with debug=False to avoid reloader issues in notebook\n",
    "        app.run(host='127.0.0.1', port=3000, debug=False, use_reloader=False)\n",
    "    finally:\n",
    "        os.chdir(original_dir)\n",
    "\n",
    "# Check if Flask is already running\n",
    "import socket\n",
    "def is_port_open(port):\n",
    "    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    result = sock.connect_ex(('127.0.0.1', port))\n",
    "    sock.close()\n",
    "    return result == 0\n",
    "\n",
    "if not is_port_open(3000):\n",
    "    # Start Flask in a background thread\n",
    "    flask_thread = threading.Thread(target=run_flask_app, daemon=True)\n",
    "    flask_thread.start()\n",
    "    \n",
    "    print(\"üöÄ Starting CatShop web application...\")\n",
    "    # Wait for Flask to start\n",
    "    for i in range(10):\n",
    "        if is_port_open(3000):\n",
    "            print(\"‚úÖ CatShop is running!\")\n",
    "            break\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Flask app didn't start in time\")\n",
    "else:\n",
    "    print(\"‚úÖ CatShop is already running!\")\n",
    "\n",
    "# Display link and embedded iframe\n",
    "display(HTML(\"\"\"\n",
    "<div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); \n",
    "            padding: 20px; border-radius: 10px; color: white; margin: 20px 0;\">\n",
    "    <h2>üê± CatShop is Ready!</h2>\n",
    "    <p>The web application is now running with your trained cat classifier.</p>\n",
    "    <p><strong>Access the app:</strong> \n",
    "       <a href=\"http://localhost:3000/test_session\" target=\"_blank\" \n",
    "          style=\"color: #FFE082; text-decoration: underline;\">\n",
    "          Open CatShop in a new tab\n",
    "       </a>\n",
    "    </p>\n",
    "    <p><strong>Features to try:</strong></p>\n",
    "    <ul>\n",
    "        <li>Search for products (e.g., \"laptop\", \"box\", \"toy\")</li>\n",
    "        <li>See cat classifications with emojis and confidence scores</li>\n",
    "        <li>Click on products and use the \"Ask the Cat\" button for cat chat</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "# Embed the app in an iframe (optional - may not work in all notebook environments)\n",
    "IFrame('http://localhost:3000/test_session', width=1000, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad0359",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That's it! You've built a complete ML system that properly demonstrates all three paradigms with the actual Gemma model. Well done! üéâ"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python (ml_lectures_env)",
   "language": "python",
   "name": "ml_lectures_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "rise": {
   "autoSlide": false,
   "autoSlideStoppable": true,
   "autolaunch": false,
   "backgroundTransition": "none",
   "center": false,
   "controls": true,
   "custom_css": "./custom.css",
   "embedded": false,
   "enable_chalkboard": true,
   "footer": "",
   "fragments": true,
   "header": "",
   "height": "100%",
   "help": true,
   "hideAddressBar": true,
   "history": true,
   "keyboard": true,
   "loop": false,
   "margin": 0.1,
   "maxScale": 2,
   "minScale": 0.2,
   "mouseWheel": true,
   "overview": true,
   "parallaxBackgroundImage": "",
   "parallaxBackgroundSize": "",
   "previewLinks": false,
   "progress": true,
   "rtl": false,
   "scroll": true,
   "showNotes": false,
   "slideNumber": true,
   "start_slideshow_at": "beginning",
   "theme": "white",
   "touch": true,
   "transition": "none",
   "transitionSpeed": "default",
   "viewDistance": 3,
   "width": "100%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
