"""
Cat Classifier module for CatShop
Integrates Gemma-3 model for cat perspective classification and chat
"""
import json
import random
from pathlib import Path

# Optional heavy deps (torch, transformers, peft). Fallback to rule-based if missing
try:
    import torch
    import torch.nn.functional as F
except Exception:
    torch = None
    F = None

# Cat categories and their properties
CAT_CATEGORIES = {
    "NAP_SURFACE": {"emoji": "üò¥", "color": "#9b59b6", "description": "Perfect for napping"},
    "HUNT_PLAY": {"emoji": "üéØ", "color": "#e74c3c", "description": "Fun to hunt and play with"},
    "TERRITORY": {"emoji": "üè∞", "color": "#3498db", "description": "Must mark as mine"},
    "GROOMING": {"emoji": "‚ú®", "color": "#f39c12", "description": "Keeps me beautiful"},
    "CONSUMPTION": {"emoji": "üçΩÔ∏è", "color": "#27ae60", "description": "Food or water"},
    "DANGER": {"emoji": "‚ö†Ô∏è", "color": "#c0392b", "description": "Scary! Must avoid!"},
    "IRRELEVANT": {"emoji": "üòë", "color": "#95a5a6", "description": "Boring human stuff"}
}

# Proxy tokens (as used in the notebook) to get clean single-token ids
PROXY_TOKENS = {
    'NAP_SURFACE': 'nap',
    'HUNT_PLAY': 'hunt',
    'TERRITORY': 'territory',
    'DANGER': 'danger',
    'CONSUMPTION': 'food',
    'GROOMING': 'groom',
    'IRRELEVANT': 'boring'
}

class CatClassifier:
    def __init__(self, model_path="models/gemma-cat-lora"):
        """Initialize the cat classifier with Gemma-3 model"""
        # Load LoRA directly from project root's models/ (generated by the notebook)
        # Example absolute: <repo>/Lecture 1 Overview/models/gemma-cat-lora
        self.model_path = (Path(__file__).parent.parent / model_path)
        self.device = (
            torch.device('cuda' if torch and torch.cuda.is_available() else 'cpu')
            if torch is not None else 'cpu'
        )
        self.model = None
        self.tokenizer = None
        # Debug counters
        self.usage_counts = {"model": 0, "rule": 0}
        self.category_counts = {k: 0 for k in CAT_CATEGORIES.keys()}
        self.using_lora = False
        self.load_model()
        # Build category token ids for proxy tokens (after tokenizer is available)
        self.category_token_ids = {}
        try:
            for cat, token_str in PROXY_TOKENS.items():
                token_ids = self.tokenizer.encode(token_str, add_special_tokens=False)
                if len(token_ids) == 0:
                    continue
                self.category_token_ids[cat] = token_ids[0]
        except Exception:
            self.category_token_ids = {}
    
    def load_model(self):
        """Load the fine-tuned Gemma-3 model"""
        try:
            # Deps
            if torch is None:
                raise ImportError("torch not available")
            from transformers import AutoTokenizer, AutoModelForCausalLM  # lazy import
            try:
                from peft import PeftModel  # lazy import
            except Exception:
                PeftModel = None

            # Load base model and tokenizer
            base_model = "google/gemma-3-270m"  # Use Gemma-3 270m
            self.tokenizer = AutoTokenizer.from_pretrained(base_model)
            
            # Check if LoRA weights exist
            if self.model_path.exists():
                # Load with LoRA
                base_model_obj = AutoModelForCausalLM.from_pretrained(
                    base_model,
                    torch_dtype=torch.float16 if (torch and torch.cuda.is_available()) else torch.float32,
                    device_map="auto" if (torch and torch.cuda.is_available()) else None
                )
                if PeftModel is not None:
                    self.model = PeftModel.from_pretrained(base_model_obj, self.model_path)
                else:
                    # If peft missing, fall back to base model
                    self.model = base_model_obj
                self.using_lora = True
                print(f"Loaded base model {base_model} with LoRA from {self.model_path}")
            else:
                # Load base without LoRA
                self.model = AutoModelForCausalLM.from_pretrained(
                    base_model,
                    torch_dtype=torch.float16 if (torch and torch.cuda.is_available()) else torch.float32,
                    device_map="auto" if (torch and torch.cuda.is_available()) else None
                )
                self.using_lora = False
                print("LoRA path not found; using base model only")
            
            # Add padding token if needed
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            if self.model is not None:
                self.model.eval()
            print(f"‚úÖ Cat model loaded successfully on {self.device}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Could not load Gemma model: {e}")
            print("Falling back to rule-based classification")
            self.model = None
            self.using_lora = False
    
    def classify_product(self, product_name, product_description=""):
        """Classify a product from cat perspective"""
        if self.model is None:
            out = self.rule_based_classify(product_name, product_description)
            out["source"] = "rule"
            self.usage_counts["rule"] += 1
            self.category_counts[out["category"]] += 1
            return out
        
        try:
            # Notebook-style prompt: predict next token after Category:
            prompt = f"Product: {product_name[:120]}\nCategory:"

            # Tokenize inputs
            inputs = self.tokenizer(prompt, return_tensors="pt", padding=False, truncation=True, max_length=256)
            if torch is not None:
                inputs = {k: v.to(self.device) for k, v in inputs.items()}

            # Get next-token logits
            if torch is not None:
                with torch.no_grad():
                    outputs = self.model(**inputs)
                    next_token_logits = outputs.logits[0, -1, :]
            else:
                raise RuntimeError("torch not available")

            # Build logits for our proxy tokens
            category_logits = []
            cats = list(CAT_CATEGORIES.keys())
            for cat in cats:
                token_id = self.category_token_ids.get(cat, None)
                if token_id is None:
                    # In rare case of missing id, assign very low logit
                    category_logits.append(torch.tensor(-1e9, device=next_token_logits.device))
                else:
                    category_logits.append(next_token_logits[token_id])

            # Convert to probabilities
            cat_logits_tensor = torch.stack(category_logits)
            probs_tensor = F.softmax(cat_logits_tensor, dim=0) if F is not None else None
            probs = {cat: probs_tensor[i].item() for i, cat in enumerate(cats)}
            
            # Get top prediction
            top_cat = max(probs, key=probs.get)
            
            result = {
                "category": top_cat,
                "confidence": probs[top_cat],
                "all_probabilities": probs,
                "emoji": CAT_CATEGORIES[top_cat]["emoji"],
                "color": CAT_CATEGORIES[top_cat]["color"],
                "description": CAT_CATEGORIES[top_cat]["description"],
                "source": "model"
            }
            self.usage_counts["model"] += 1
            self.category_counts[top_cat] += 1
            return result
            
        except Exception as e:
            print(f"Model inference error: {e}")
            out = self.rule_based_classify(product_name, product_description)
            out["source"] = "rule"
            self.usage_counts["rule"] += 1
            self.category_counts[out["category"]] += 1
            return out
    
    def rule_based_classify(self, product_name, product_description=""):
        """Fallback rule-based classification"""
        text = f"{product_name} {product_description}".lower()
        
        # Simple keyword matching
        if any(w in text for w in ['laptop', 'computer', 'keyboard', 'monitor', 'bed', 'cushion', 'pillow']):
            category = 'NAP_SURFACE'
        elif any(w in text for w in ['toy', 'ball', 'feather', 'play', 'mouse', 'laser']):
            category = 'HUNT_PLAY'
        elif any(w in text for w in ['furniture', 'chair', 'sofa', 'scratch', 'post']):
            category = 'TERRITORY'
        # Mark DANGER only for clearly scary/loud devices; avoid broad words like 'cleaner' alone
        elif (
            any(w in text for w in ['vacuum', 'leaf blower', 'chainsaw', 'power saw', 'drill', 'jackhammer'])
            or ('blender' in text and 'kitchen' in text)
            or ('cleaner' in text and ('vacuum' in text or 'steam' in text))
            or 'air compressor' in text
            or 'pressure washer' in text
        ):
            category = 'DANGER'
        elif any(w in text for w in ['food', 'treat', 'bowl', 'water', 'kibble']):
            category = 'CONSUMPTION'
        elif any(w in text for w in ['brush', 'groom', 'shampoo', 'comb']):
            category = 'GROOMING'
        else:
            category = 'IRRELEVANT'
        
        # Add some randomness to confidence for realism
        confidence = 0.7 + random.random() * 0.25
        
        # Create fake probability distribution
        probs = {cat: 0.05 for cat in CAT_CATEGORIES}
        probs[category] = confidence
        remaining = 1.0 - confidence - 0.05 * (len(CAT_CATEGORIES) - 1)
        for cat in probs:
            if cat != category:
                probs[cat] += remaining / (len(CAT_CATEGORIES) - 1)
        
        return {
            "category": category,
            "confidence": confidence,
            "all_probabilities": probs,
            "emoji": CAT_CATEGORIES[category]["emoji"],
            "color": CAT_CATEGORIES[category]["color"],
            "description": CAT_CATEGORIES[category]["description"]
        }
    
    def chat_about_product(self, product_name, user_question):
        """Have a cat conversation about a product"""
        if self.model is None:
            return self.rule_based_chat(product_name, user_question)
        
        try:
            prompt = f"""You are a cat evaluating products for online shopping. Be playful, mention cat behaviors like napping, hunting, scratching, and give opinions from a cat's perspective. Keep responses under 100 words.

Product: {product_name}
Human: {user_question}
Cat:"""
            
            inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=256)
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Extract just the cat's response
            if "Cat:" in response:
                response = response.split("Cat:")[-1].strip()
            else:
                response = response[len(prompt):].strip()
            
            return response
            
        except Exception as e:
            print(f"Chat generation error: {e}")
            return self.rule_based_chat(product_name, user_question)
    
    def rule_based_chat(self, product_name, user_question):
        """Fallback rule-based chat responses"""
        classification = self.rule_based_classify(product_name)
        category = classification["category"]
        
        responses = {
            "NAP_SURFACE": [
                f"*yawns* Oh, {product_name} looks purr-fect for my afternoon naps! The surface seems warm and comfortable. I claim it as my new throne!",
                f"Meow! This {product_name} would make an excellent napping spot. Much better than that silly cat bed the humans bought me.",
                f"*stretches* I can already imagine curling up on this {product_name}. The humans won't mind if I shed a little fur on it, right?"
            ],
            "HUNT_PLAY": [
                f"*eyes dilate* Ooh! {product_name} triggers my hunting instincts! I must pounce immediately!",
                f"*wiggles butt* This {product_name} looks like so much fun to chase! My inner predator is activated!",
                f"Mrow! Finally, something exciting! I shall hunt this {product_name} at 3 AM when the humans are sleeping."
            ],
            "TERRITORY": [
                f"*sniffs* This {product_name} clearly needs my scent on it. It's mine now, humans!",
                f"Hmm, {product_name} would be perfect for scratching. Much better than that expensive scratching post!",
                f"I must mark this {product_name} as part of my territory. *rubs cheek against screen*"
            ],
            "DANGER": [
                f"HISS! {product_name} is scary! *runs away and hides under the bed*",
                f"*ears flatten* No no no! Keep that terrifying {product_name} away from me!",
                f"*puffs up* That {product_name} is my mortal enemy! I shall observe it from a safe distance... like another room."
            ],
            "CONSUMPTION": [
                f"*licks lips* Is this {product_name} food-related? I'm suddenly interested... unless it's medicine disguised as a treat!",
                f"Meow? Does {product_name} contain treats? I deserve treats for being magnificent!",
                f"*sniffs suspiciously* This {product_name} better be the good stuff, not that healthy food the vet recommended."
            ],
            "GROOMING": [
                f"*grooms paw* I suppose {product_name} is acceptable for maintaining my glorious coat.",
                f"Meh, {product_name} might help me look even more beautiful, if that's even possible.",
                f"*yawns* Do I really need {product_name}? I'm already purr-fect! But I'll allow it."
            ],
            "IRRELEVANT": [
                f"*yawns* {product_name}? How boring. Wake me when there's something actually interesting.",
                f"*turns away* Another silly human thing. This {product_name} has nothing to do with me.",
                f"Mrow... {product_name} is just human nonsense. Where are my treats?"
            ]
        }
        
        import random
        return random.choice(responses.get(category, responses["IRRELEVANT"]))

# Global instance
cat_classifier = None

def get_cat_classifier():
    """Get or create the global cat classifier instance"""
    global cat_classifier
    if cat_classifier is None:
        cat_classifier = CatClassifier()
    return cat_classifier