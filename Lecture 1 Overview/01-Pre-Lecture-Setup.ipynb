{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a942834",
   "metadata": {},
   "source": [
    "# Machine Learning Course - Pre-Lecture Verification Notebook\n",
    "\n",
    "**Run this notebook AFTER completing the terminal setup instructions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398f549",
   "metadata": {},
   "source": [
    "## System Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f399d069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è SYSTEM INFORMATION\n",
      "============================================================\n",
      "Python Version: 3.9.6 (default, Apr 30 2025, 02:07:17) \n",
      "[Clang 17.0.0 (clang-1700.0.13.5)]\n",
      "Platform: macOS-15.5-arm64-arm-64bit\n",
      "Python Executable: /Users/ming/Dropbox/learn-ml-by-building/ml_lectures_env/bin/python\n",
      "Available Memory: 44.09 GB\n",
      "Total Memory: 96.00 GB\n",
      "GPU Available: ‚ùå CPU only (this is fine for the course!)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üñ•Ô∏è SYSTEM INFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"Python Executable: {sys.executable}\")\n",
    "\n",
    "# Check available memory\n",
    "try:\n",
    "    import psutil\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Available Memory: {memory.available / (1024**3):.2f} GB\")\n",
    "    print(f\"Total Memory: {memory.total / (1024**3):.2f} GB\")\n",
    "except ImportError:\n",
    "    print(\"Memory check: psutil not installed\")\n",
    "\n",
    "# Check GPU availability\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Available: ‚úÖ {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"GPU Available: ‚ùå CPU only (this is fine for the course!)\")\n",
    "except ImportError:\n",
    "    print(\"GPU Check: PyTorch not installed\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ada1b",
   "metadata": {},
   "source": [
    "## Verify Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f5aca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç VERIFYING PACKAGE IMPORTS\n",
      "============================================================\n",
      "‚úÖ PyTorch                   v2.7.1\n",
      "‚úÖ Transformers              v4.55.2\n",
      "‚úÖ Sentence-Transformers     v5.1.0\n",
      "‚úÖ Datasets                  v4.0.0\n",
      "‚úÖ NumPy                     v2.0.2\n",
      "‚úÖ Pandas                    v2.3.1\n",
      "‚úÖ Scikit-learn              v1.6.1\n",
      "‚úÖ Matplotlib                v3.9.4\n",
      "‚úÖ Seaborn                   v0.13.2\n",
      "‚úÖ TQDM                      v4.67.1\n",
      "‚úÖ Accelerate                v1.10.0\n",
      "\n",
      "üéâ All packages imported successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "print(\"üîç VERIFYING PACKAGE IMPORTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "packages_to_check = {\n",
    "    \"torch\": \"PyTorch\",\n",
    "    \"transformers\": \"Transformers\",\n",
    "    \"sentence_transformers\": \"Sentence-Transformers\",\n",
    "    \"datasets\": \"Datasets\",\n",
    "    \"numpy\": \"NumPy\",\n",
    "    \"pandas\": \"Pandas\",\n",
    "    \"sklearn\": \"Scikit-learn\",\n",
    "    \"matplotlib\": \"Matplotlib\",\n",
    "    \"seaborn\": \"Seaborn\",\n",
    "    \"tqdm\": \"TQDM\",\n",
    "    \"accelerate\": \"Accelerate\",\n",
    "}\n",
    "\n",
    "all_imported = True\n",
    "for module_name, description in packages_to_check.items():\n",
    "    try:\n",
    "        module = importlib.import_module(module_name)\n",
    "        version = getattr(module, \"__version__\", \"unknown\")\n",
    "        print(f\"‚úÖ {description:25s} v{version}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå {description:25s} - Not found\")\n",
    "        all_imported = False\n",
    "\n",
    "if all_imported:\n",
    "    print(\"\\nüéâ All packages imported successfully!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some packages missing. Check terminal setup instructions.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3ff00",
   "metadata": {},
   "source": [
    "## Download and Verify Gemma-3-270m Model\n",
    "\n",
    "Now let's download the Gemma-3-270m model. This is a small model (~550MB) that we'll use in class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dab49ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1faed7cabf1b4855a75137e830dc00be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Login to Hugging Face (opens a prompt in notebook)\n",
    "# Make sure you've accepted the model license first:\n",
    "# https://huggingface.co/google/gemma-3-270m\n",
    "from huggingface_hub import login\n",
    "login()  # paste your HF token here; token is stored for future use\n",
    "\n",
    "# Optional: configure local cache (edit path if you want):\n",
    "# import os\n",
    "# os.environ[\"HF_HOME\"] = \"/Users/jinming/.cache/huggingface\"\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"/Users/jinming/.cache/huggingface/transformers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c45b8e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local model dir: /Users/ming/Dropbox/learn-ml-by-building/Lecture 1 Overview/models/gemma-3-270m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5e0c2940064c9ca348b97389c1fa58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a09f89f3a844b7e84769122ce4b02cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe99458fb9f40dca19634d3284491dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Files downloaded to: /Users/ming/Dropbox/learn-ml-by-building/Lecture 1 Overview/models/gemma-3-270m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 640, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear(in_features=640, out_features=1024, bias=False)\n",
       "          (k_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=640, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (up_proj): Linear(in_features=640, out_features=2048, bias=False)\n",
       "          (down_proj): Linear(in_features=2048, out_features=640, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((640,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=640, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explicit local download (no symlinks) + load from local path\n",
    "\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download  # pip install huggingface_hub\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# 1) Choose a lecture-local folder\n",
    "local_dir = (Path.cwd() / \"models\" / \"gemma-3-270m\").resolve()\n",
    "local_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Local model dir:\", local_dir)\n",
    "\n",
    "# 2) Ensure you've accepted the license and are logged in (run once in a previous cell):\n",
    "# from huggingface_hub import login\n",
    "# login()  # paste your HF token\n",
    "\n",
    "# 3) Download repo contents directly into local_dir (no symlinks)\n",
    "snapshot_download(\n",
    "    repo_id=\"google/gemma-3-270m\",\n",
    "    local_dir=str(local_dir),\n",
    "    local_dir_use_symlinks=False,  # ensures real files are written here\n",
    ")\n",
    "print(\"‚úÖ Files downloaded to:\", local_dir)\n",
    "\n",
    "# 4) Load from local path\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_dir,\n",
    "    torch_dtype=torch.float32,      # adjust if you have GPU: torch.float16\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33c1766b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Ä¢ LM logits shape: torch.Size([1, 6, 262144])\n",
      "Sample: This is a laptop computer with a 15.6 inch screen. It has a 16GB of RAM and a 1TB of storage. It has a 108\n",
      "‚úÖ Gemma-3-270m loaded from local directory.\n"
     ]
    }
   ],
   "source": [
    "# 4) Load from local path\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_dir, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_dir,\n",
    "    torch_dtype=torch.float32,      # adjust if you have GPU: torch.float16\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# 5) Quick smoke test\n",
    "text = \"This is a laptop computer\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(\"‚Ä¢ LM logits shape:\", outputs.logits.shape)\n",
    "\n",
    "gen_ids = model.generate(**inputs, max_length=40, do_sample=False)\n",
    "print(\"Sample:\", tokenizer.decode(gen_ids[0], skip_special_tokens=True))\n",
    "print(\"‚úÖ Gemma-3-270m loaded from local directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754264c",
   "metadata": {},
   "source": [
    "## Verify WebShop Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56b1f069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ VERIFYING: items_ins_v2_1000.json\n",
      "============================================================\n",
      "‚úÖ Dataset loaded. Product IDs: 1000\n",
      "\n",
      "üìä Stats:\n",
      "  ‚Ä¢ Entries with attributes: 1000\n",
      "  ‚Ä¢ Avg attributes per entry: 2.83\n",
      "  ‚Ä¢ Entries with instruction: 415\n",
      "\n",
      "üìã Sample Entry:\n",
      "  ‚Ä¢ ID: B08GFNJN5R\n",
      "  ‚Ä¢ attributes: []\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify WebShop \"items_ins_v2_1000.json\" (attributes/instructions)\n",
    "import json, os\n",
    "from collections import Counter\n",
    "\n",
    "print(\"üì¶ VERIFYING: items_ins_v2_1000.json\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# If running the notebook from \"Lecture 1 Overview/\", this is correct:\n",
    "data_path = \"data/items_ins_v2_1000.json\"\n",
    "# If running from repo root, use:\n",
    "# data_path = \"Lecture 1 Overview/data/items_ins_v2_1000.json\"\n",
    "\n",
    "if os.path.exists(data_path):\n",
    "    with open(data_path, \"r\") as f:\n",
    "        items = json.load(f)\n",
    "\n",
    "    print(f\"‚úÖ Dataset loaded. Product IDs: {len(items)}\")\n",
    "\n",
    "    # Expect top-level dict keyed by IDs -> dicts with attributes, optional instruction fields\n",
    "    has_instruction = sum(1 for v in items.values() if \"instruction\" in v)\n",
    "    has_attrs = sum(1 for v in items.values() if \"attributes\" in v and isinstance(v[\"attributes\"], list))\n",
    "    attr_lengths = [len(v.get(\"attributes\", [])) for v in items.values()]\n",
    "    avg_attrs = (sum(attr_lengths) / len(attr_lengths)) if attr_lengths else 0.0\n",
    "\n",
    "    print(\"\\nüìä Stats:\")\n",
    "    print(f\"  ‚Ä¢ Entries with attributes: {has_attrs}\")\n",
    "    print(f\"  ‚Ä¢ Avg attributes per entry: {avg_attrs:.2f}\")\n",
    "    print(f\"  ‚Ä¢ Entries with instruction: {has_instruction}\")\n",
    "\n",
    "    # Show a sample entry\n",
    "    first_key = next(iter(items))\n",
    "    sample = items[first_key]\n",
    "    print(\"\\nüìã Sample Entry:\")\n",
    "    print(f\"  ‚Ä¢ ID: {first_key}\")\n",
    "    print(f\"  ‚Ä¢ attributes: {sample.get('attributes', [])[:8]}\")\n",
    "    if \"instruction\" in sample:\n",
    "        print(f\"  ‚Ä¢ instruction: {sample['instruction'][:80]}...\")\n",
    "        print(f\"  ‚Ä¢ instruction_attributes: {sample.get('instruction_attributes', [])[:8]}\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset not found at {data_path}\")\n",
    "    print(\"   Tip: ensure you run this notebook from 'Lecture 1 Overview/' or fix the relative path.\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "626af4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ items_shuffle_1000.json loaded: 1000\n",
      "üìã Sample name: Vhomes Lights Reclaimed Wood Console Table The Genessis Collection Sofa-Tables\n"
     ]
    }
   ],
   "source": [
    "# Verify \"items_shuffle_1000.json\" (richer fields, may include name/price/etc.)\n",
    "import json, os\n",
    "from collections import Counter\n",
    "\n",
    "path2 = \"data/items_shuffle_1000.json\"  # or \"Lecture 1 Overview/data/...\" from repo root\n",
    "if os.path.exists(path2):\n",
    "    with open(path2, \"r\") as f:\n",
    "        items2 = json.load(f)\n",
    "\n",
    "    print(f\"‚úÖ items_shuffle_1000.json loaded: {len(items2)}\")\n",
    "    # Decide if it's a list or dict\n",
    "    if isinstance(items2, dict):\n",
    "        values = items2.values()\n",
    "    else:\n",
    "        values = items2\n",
    "\n",
    "    names = [x.get(\"name\") for x in values if isinstance(x, dict)]\n",
    "    print(\"üìã Sample name:\", next((n for n in names if n), \"N/A\"))\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è Not found: {path2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e4c47",
   "metadata": {},
   "source": [
    "## Test ML Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15dbbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "print(\"üß† TESTING ML COMPONENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test scikit-learn\n",
    "X = np.random.rand(100, 10)\n",
    "y = np.random.randint(0, 3, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train, y_train)\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(f\"‚úÖ Scikit-learn KNN: {accuracy:.2f} accuracy\")\n",
    "\n",
    "# Test sentence transformers\n",
    "print(\"\\nLoading sentence transformer (first run may take a minute)...\")\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "test_sentences = [\"laptop computer\", \"cat toy\", \"fiction book\"]\n",
    "embeddings = sentence_model.encode(test_sentences)\n",
    "print(f\"‚úÖ Sentence embeddings: shape {embeddings.shape}\")\n",
    "\n",
    "# Test clustering\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n",
    "print(f\"‚úÖ KMeans clustering: {labels}\")\n",
    "\n",
    "# Test matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "ax.scatter([1, 2, 3], [1, 4, 2])\n",
    "ax.set_title(\"Test Plot\")\n",
    "plt.close()\n",
    "print(\"‚úÖ Matplotlib: working\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece97484",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "**\"Cannot download Gemma model\"**\n",
    "- Accept license at: https://huggingface.co/google/gemma-3-270m\n",
    "- Run: `huggingface-cli login` in terminal\n",
    "\n",
    "**\"Module not found\"**\n",
    "- Ensure virtual environment is activated\n",
    "- Re-run package installation from terminal setup\n",
    "\n",
    "**\"Dataset not found\"**\n",
    "- Check file exists in `Lecture 1 Overview/data/`\n",
    "- Re-run download commands from terminal setup\n",
    "\n",
    "**\"Out of memory\"**\n",
    "- Close other applications\n",
    "- The setup only needs ~2GB RAM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_lectures_env)",
   "language": "python",
   "name": "ml_lectures_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
